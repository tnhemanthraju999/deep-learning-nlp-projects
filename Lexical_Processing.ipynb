{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnhemanthraju999/deep-learning-nlp-projects/blob/main/Lexical_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55a0979-88ca-4df1-8ff0-ba82de9b76d0",
      "metadata": {
        "id": "d55a0979-88ca-4df1-8ff0-ba82de9b76d0"
      },
      "source": [
        "# Email Summarisation Challenge\n",
        "\n",
        "Imagine your inbox after a long weekend:\n",
        "- 20 different people have replied to the same thread.\n",
        "- Some replies are short “Thanks.”\n",
        "- Some are long updates, forwards, or clarifications.\n",
        "- By Monday morning, you’re staring at an endless <b>email chain</b>.\n",
        "\n",
        "<b>&rarr; <i>Your brain wants a summary.</i></b>\n",
        "\n",
        "That’s exactly what our dataset gives us: <b>raw email threads + human-written summaries</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e635fef1-d834-4064-b559-147b5ad254f7",
      "metadata": {
        "id": "e635fef1-d834-4064-b559-147b5ad254f7"
      },
      "source": [
        "## What’s Inside the Dataset?\n",
        "\n",
        "Think of the dataset as a <b>two-part diary of conversations:</b>\n",
        "\n",
        "<b>PART I - The Full Story (Email Thread Details)</b>\n",
        "\n",
        "<ul>\n",
        "    <li>Every email in a thread: who sent it, when, to whom, and the full body text.</li>\n",
        "    <li>Columns you’ll meet:</li>\n",
        "    <ul style=\"list-style-type:circle\">\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">thread_id</span> → the conversation ID</li>\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">subject</span> → the topic line</li>\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">timestamp</span> → when the message was sent</li>\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">from</span> → the sender</li>\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">to</span> → the recipients</li>\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">body</span> → the actual text (our playground!)</li>\n",
        "    </ul>\n",
        "</ul>\n",
        "\n",
        "This is where Lexical, Syntactic, and Semantic Processing will happen.\n",
        "\n",
        "<b>PART II - The Short Story (Email Thread Summaries)</b>\n",
        "\n",
        "<ul>\n",
        "    <li>Human annotators have already done the hard work of reading messy threads and writing clean summaries.</li>\n",
        "    <li>Columns you’ll meet:</li>\n",
        "    <ul style=\"list-style-type:circle\">\n",
        "        <li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">thread_id</span> → matches with the details file</li>\n",
        "    \t<li><span style=\"border-radius: 4px; background-color: rgb(241, 241, 241); padding: 2px;\">summary</span> → the concise version</li>\n",
        "    </ul>\n",
        "</ul>\n",
        "\n",
        "This is our <b>gold standard</b> to check how close our models come to humans."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4991ca47-5c17-4a04-a396-8109047bf84d",
      "metadata": {
        "id": "4991ca47-5c17-4a04-a396-8109047bf84d"
      },
      "source": [
        "### The Scale of It All\n",
        "<ul>\n",
        "    <li><b>Threads:</b> 4,167</li>\n",
        "\t<li><b>Emails:</b> 21,684</li>\n",
        "\t<li><b>Language:</b> English</li>\n",
        "</ul>\n",
        "\n",
        "Not too small (so models learn), not too huge (so we don’t drown).\n",
        "Perfect for experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a4a4870-f8ad-415a-8c3f-0b89f69363d6",
      "metadata": {
        "id": "1a4a4870-f8ad-415a-8c3f-0b89f69363d6"
      },
      "source": [
        "#### Important Note for Our Journey\n",
        "While the dataset is rich and large, we won’t unleash all of it at once.\n",
        "<ul>\n",
        "\t<li>In the <b>early stages (Lexical & Syntactic Processing)</b>, we’ll use a <b>smaller slice</b> of the data — short email threads — to keep things light and easy to follow.</li>\n",
        "\t<li>As we move into <b>Semantic Processing and Summarisation,</b> we’ll gradually scale up to bigger chunks.</li>\n",
        "</ul>\n",
        "\n",
        "This way, you’ll see how techniques work on <b>tiny examples first</b> … and then apply them to the <b>real-world scale</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73352395-a588-4c14-8369-126b242846c2",
      "metadata": {
        "id": "73352395-a588-4c14-8369-126b242846c2"
      },
      "source": [
        "### Why This Dataset?\n",
        "\n",
        "This dataset allows us to walk step-by-step through the layers of NLP:\n",
        "<ul>\n",
        "    <li><b>Lexical:</b> cleaning, tokenising, fixing spellings.</li>\n",
        "\t<li><b>Syntactic:</b> POS tagging, parsing, grammar structures.</li>\n",
        "\t<li><b>Semantic:</b> meaning, word senses, entities, roles.</li>\n",
        "\t<li><b>Conceptual:</b> finally, creating summaries that make sense.</li>\n",
        "</ul>\n",
        "\n",
        "It’s like peeling an onion — one layer at a time until the final flavour emerges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4338a7e5-7fa8-4715-b39a-bc7b5c9724f3",
      "metadata": {
        "id": "4338a7e5-7fa8-4715-b39a-bc7b5c9724f3"
      },
      "source": [
        "# Load and Peek into the Dataset\n",
        "\n",
        "Before we start transforming the text, let’s first load the dataset and see what our raw email data looks like.\n",
        "\n",
        "We’ll:\n",
        "<ol>\n",
        "\t<li>Import the JSON files.</li>\n",
        "\t<li>Take a quick peek at the structure.</li>\n",
        "\t<li>Sample 10 random emails to get a flavour of the text we’ll be working with.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89649137-b52b-41e9-a051-bdf0c6fe0419",
      "metadata": {
        "id": "89649137-b52b-41e9-a051-bdf0c6fe0419"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25262167-69eb-429d-9713-9e57cf8f9eff",
      "metadata": {
        "id": "25262167-69eb-429d-9713-9e57cf8f9eff"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Loading the JSON data\n",
        "email_data = json.load(open(\"/content/email_thread_details.json\"))\n",
        "email_summary = json.load(open(\"/content/email_thread_summaries.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333ac4d8-e968-4cff-bcde-7155628858c2",
      "metadata": {
        "id": "333ac4d8-e968-4cff-bcde-7155628858c2"
      },
      "source": [
        "## Merge Email Subject and Body and Unify the Summary with Email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e60459d9-bfe6-4874-955a-1918f291a015",
      "metadata": {
        "id": "e60459d9-bfe6-4874-955a-1918f291a015"
      },
      "outputs": [],
      "source": [
        "# Merge Subject and Body\n",
        "def merge_subject_and_body(thread):\n",
        "    return f\"\"\"\n",
        "    SUBJECT- {thread[\"subject\"]}\n",
        "\n",
        "    BODY- {thread[\"body\"]}\n",
        "    \"\"\"\n",
        "\n",
        "# Unify the data by `thread_id`\n",
        "email_dataset = {threads[\"thread_id\"]: {\"email\": merge_subject_and_body(threads)} for threads in email_data}\n",
        "for summary in email_summary:\n",
        "    email_dataset[summary[\"thread_id\"]][\"summary\"] = summary[\"summary\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7618f6-8991-4479-9bb3-2c8ecafc6c18",
      "metadata": {
        "id": "fa7618f6-8991-4479-9bb3-2c8ecafc6c18",
        "scrolled": true
      },
      "source": [
        "## Subsample 10 datapoints and peek into them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a9653f6-3eae-473c-9330-df5f76e03f10",
      "metadata": {
        "id": "1a9653f6-3eae-473c-9330-df5f76e03f10"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sampled_keys = random.sample(list(email_dataset.keys()), 10)\n",
        "\n",
        "sub_email_dataset = {k: email_dataset[k] for k in sampled_keys}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f033db14-52a6-4e7a-9f0b-a8408ef2cb3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f033db14-52a6-4e7a-9f0b-a8408ef2cb3d",
        "outputId": "7bba9651-9243-4b13-940d-40c29940a0cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[244, 3007, 2966, 3034, 562, 2081, 3814, 772, 1650, 1640]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sampled_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7c16b0e2-648a-4e3b-9327-ad49f55964d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c16b0e2-648a-4e3b-9327-ad49f55964d3",
        "outputId": "11847fd3-a999-400a-f340-9e3affca75d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "Original Email - \n",
            "-------------------------\n",
            "\n",
            "    SUBJECT- FW: Welcome to UBS meeting tommorrow 10.15 am @ the Houstonian -\n",
            " URGENT REQUIRES IMMEDIATE ACTION\n",
            "\n",
            "    BODY- \n",
            "\n",
            " -----Original Message-----\n",
            "From: \tDavies, Neil  \n",
            "Sent:\tTuesday, January 22, 2002 5:37 PM\n",
            "To:\tOxley, David; Kitchen, Louise; Fitzpatrick, Amy; Slone, Jeanie; Curless, Amanda; Weatherstone, Mary; Clyatt, Julie; Beck, Sally\n",
            "Cc:\tDonoghue, Sean ; Woods, Steve\n",
            "Subject:\tFW: Welcome to UBS meeting tommorrow 10.15 am @ the Houstonian - URGENT REQUIRES IMMEDIATE ACTION\n",
            "Importance:\tHigh\n",
            "\n",
            "\n",
            "An important meeting  will be held tommorrow for all employees who\n",
            "\n",
            "1) have accepted offers \n",
            "2) intend to accept offers ( to the best of their knowledge) but who have issues that need to be resolved\n",
            "\n",
            "All employees in these categories should attend\n",
            "\n",
            "Buses will be provided for those who do not have their own transport and will pick up from the south side of the north building - please assemble in the Java plaza area from 8.30am  to 9.30am ( last bus @ 9.30 am )\n",
            "\n",
            "The meeting will last until 1pm approximately and will be presented by senior managers from UBS on the firm and it's strategy.\n",
            "\n",
            "There will be breakfast available at the Hotel from 9am to 10am and there will be sandwiches afterwards.  \n",
            "\n",
            "If travelling independently please make sure you leave sufficient time to park (it will be very busy there ) and get to the Grand ballroom\n",
            "\n",
            "Employees need to bring their id's or drivers licence with them to gain access to the meeting\n",
            "\n",
            "I'm sorry for the short notice on this - please call out to anyone who should be attending who will not get this message in time \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Neil Daves\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "-------------------------\n",
            "Email Summary - \n",
            "-------------------------\n",
            "There is an urgent meeting tomorrow for employees who have accepted offers or intend to accept offers but have unresolved issues. Buses will be provided for transportation, and employees should assemble in the Java plaza area. The meeting will be presented by senior managers from UBS and will cover the firm's strategy. Breakfast and sandwiches will be available. Employees should bring their IDs or driver's licenses for access. Neil Daves apologizes for the short notice and asks for help in notifying those who may not receive the message in time. Amy provides additional details for the meeting.\n"
          ]
        }
      ],
      "source": [
        "print(\"-\"*25, \"Original Email - \", \"-\"*25, sub_email_dataset[sampled_keys[0]][\"email\"], sep=\"\\n\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"-\"*25, \"Email Summary - \", \"-\"*25, sub_email_dataset[sampled_keys[0]][\"summary\"], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08a7520-27fb-4189-9ba2-1c8ea159f4d9",
      "metadata": {
        "id": "e08a7520-27fb-4189-9ba2-1c8ea159f4d9"
      },
      "source": [
        "# Lexical Processing – The First Layer\n",
        "\n",
        "When humans read an email, we don’t immediately think about deep grammar or meaning.\n",
        "Our first instinct is simple: <b>clean up the text and break it down into words and sentences.</b>\n",
        "\n",
        "That’s what <b>Lexical Processing</b> is all about.\n",
        "It’s the foundation step of NLP — preparing raw, messy email text into a <b>machine-readable format</b>.\n",
        "\n",
        "Think of it as:\n",
        "<ul>\n",
        "\t<li>Removing the noise</li>\n",
        "\t<li>Normalising the style</li>\n",
        "\t<li>Breaking text into basic building blocks (words, sentences, expressions)</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14a0e41-0211-4f80-b9a1-ffb9df5d28bb",
      "metadata": {
        "id": "f14a0e41-0211-4f80-b9a1-ffb9df5d28bb"
      },
      "source": [
        "<b><i>What We’ll Do in Lexical Processing</i></b>\n",
        "<ol>\n",
        "\t<li>Text Normalisation → make the text consistent (lowercase, remove special chars).</li>\n",
        "\t<li>Tokenisation → split into words and sentences.</li>\n",
        "\t<li>Stopword Removal → filter out “the, is, at…” that add little meaning.</li>\n",
        "\t<li>Morphological Analysis → study roots, prefixes, suffixes, inflections.</li>\n",
        "\t<li>Stemming & Lemmatisation → reduce words to their base form.</li>\n",
        "    <li>Spell Correction → handle typos using edit distance & noisy channel models.</li>\n",
        "</ol>\n",
        "\n",
        "Each of these steps will change raw email bodies into cleaner, structured text that can fuel the higher-level tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b2c310-04a6-45e9-ae90-e88590c6e2f1",
      "metadata": {
        "id": "50b2c310-04a6-45e9-ae90-e88590c6e2f1"
      },
      "source": [
        "## Text Normalisation\n",
        "\n",
        "Make email text consistent and lighter for downstream steps by removing artifacts and standardising form.\n",
        "\n",
        "<b>What we’ll normalise</b>\n",
        "<ul>\n",
        "    <li>lowercase</li>\n",
        "    <li>remove URLs, email addresses, email mentions</li>\n",
        "    <li>strip quoted replies (> lines), “On … wrote:” headers</li>\n",
        "    <li>trim common signature blocks (-- …)</li>\n",
        "    <li>squash extra whitespace</li>\n",
        "    <li>(keep punctuation for now; we’ll revisit in tokenisation)</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2ae51647-27dc-4597-8294-a022c507c8fd",
      "metadata": {
        "id": "2ae51647-27dc-4597-8294-a022c507c8fd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# ---- Regexes for common artifacts ----\n",
        "\n",
        "# REGEX for URL\n",
        "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "\n",
        "# REGEX for EMAIL\n",
        "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
        "\n",
        "# REGEX for Quoted Lines\n",
        "QUOTE_RE = re.compile(r\"(^>.*?$)\", flags=re.MULTILINE)\n",
        "\n",
        "# REGEX for Mentions\n",
        "MENTION_RE = re.compile(r'@\\w+')\n",
        "\n",
        "# REGEX for email thread headers\n",
        "FWD_REPLY_RE = re.compile(r\"(?im)^(on .+? wrote:|from:.+|sent:.+|subject:.+)$\")\n",
        "\n",
        "# REGEX for Signatures of the Emails\n",
        "SIGNATURE_RE = re.compile(r\"(?ms)\\n--\\s*\\n.*$\")\n",
        "\n",
        "\n",
        "# Function to Normalise the email texts\n",
        "def normalize_email_text(text):\n",
        "    \"\"\"Apply a simple, explainable normalisation for teaching.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    t = text.lower()\n",
        "\n",
        "    t = URL_RE.sub(\" \", t)\n",
        "    t = EMAIL_RE.sub(\" \", t)\n",
        "    t = MENTION_RE.sub(\" \", t)\n",
        "    t = QUOTE_RE.sub(\" \", t)\n",
        "    t = FWD_REPLY_RE.sub(\" \", t)\n",
        "    t = SIGNATURE_RE.sub(\" \", t)\n",
        "\n",
        "    # Remove “-----Original Message-----” sections\n",
        "    t = re.sub(r\"-{2,}.*?original message.*?-{2,}\", \" \", t, flags=re.DOTALL)\n",
        "    # Remove phone/fax numbers\n",
        "    t = re.sub(r\"\\b\\d{3}[-.\\)]?\\d{3}[-.]?\\d{4}\\b\", \" \", t)\n",
        "    # collapse whitespace\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d70d261-45ff-45fa-a5e7-186fd7b9ce25",
      "metadata": {
        "id": "2d70d261-45ff-45fa-a5e7-186fd7b9ce25"
      },
      "source": [
        "### Let's apply the function on all the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cb5fdc78-2482-4e08-b8bc-05f13b6e3195",
      "metadata": {
        "id": "cb5fdc78-2482-4e08-b8bc-05f13b6e3195"
      },
      "outputs": [],
      "source": [
        "for thread_id, data in sub_email_dataset.items():\n",
        "    sub_email_dataset[thread_id][\"normalised_email\"] = normalize_email_text(data[\"email\"])\n",
        "    sub_email_dataset[thread_id][\"normalised_summary\"] = normalize_email_text(data[\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96abcb7-49dd-4a7f-9ec9-1d6dac059b94",
      "metadata": {
        "id": "d96abcb7-49dd-4a7f-9ec9-1d6dac059b94"
      },
      "source": [
        "### Let's understand the impact of Text Normalisation\n",
        "\n",
        "Let us analyse the difference of the text length before and after normalisation\n",
        "\n",
        "We'll do this using 3 aspects\n",
        "<ul>\n",
        "    <li>Average text length before normalisation</li>\n",
        "    <li>Average text length after normalisation</li>\n",
        "    <li>Average text length difference</li>\n",
        "    <li>Average text length difference percentage</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c65852b9-19d3-4d10-98e1-0b0c1d2b35ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c65852b9-19d3-4d10-98e1-0b0c1d2b35ac",
        "outputId": "c50d09fa-2f07-413c-f76c-f6fdce489ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Text Length before Normalisation: 1708\n",
            "Average Text Length after Normalisation 1398\n",
            "Average Text Length Reduction: 310\n",
            "Average Text Length Reduction (%age): 20.72\n"
          ]
        }
      ],
      "source": [
        "analysis = {\n",
        "    \"before\": [],\n",
        "    \"after\": [],\n",
        "    \"difference\": [],\n",
        "    \"difference_perc\": []\n",
        "}\n",
        "\n",
        "for thread_id, data in sub_email_dataset.items():\n",
        "    analysis[\"before\"].append(len(data[\"email\"]))\n",
        "    analysis[\"after\"].append(len(data[\"normalised_email\"]))\n",
        "    analysis[\"difference\"].append(len(data[\"email\"])-len(data[\"normalised_email\"]))\n",
        "    analysis[\"difference_perc\"].append((len(data[\"email\"])-len(data[\"normalised_email\"])) / len(data[\"email\"]))\n",
        "\n",
        "\n",
        "print(\"Average Text Length before Normalisation:\", round((sum(analysis[\"before\"])/len(analysis[\"before\"]))))\n",
        "print(\"Average Text Length after Normalisation\", round((sum(analysis[\"after\"])/len(analysis[\"after\"]))))\n",
        "print(\"Average Text Length Reduction:\", round((sum(analysis[\"difference\"])/len(analysis[\"difference\"]))))\n",
        "print(\"Average Text Length Reduction (%age):\", round((sum(analysis[\"difference_perc\"])/len(analysis[\"difference_perc\"]))*100, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96496966-9c82-4bf1-b134-f5ffe1f04356",
      "metadata": {
        "id": "96496966-9c82-4bf1-b134-f5ffe1f04356"
      },
      "source": [
        "## Tokenisation\n",
        "\n",
        "Split normalised email text into <b>sentences</b> and <b>words</b>, and optionally glue common multi-word expressions into single tokens for stability downstream.\n",
        "\n",
        "<b>What we’ll do</b>\n",
        "<ul>\n",
        "    <li>Sentence tokenisation → sent_tokenize</li>\n",
        "\t<li>Word tokenisation → word_tokenize + keep alphabetic tokens</li>\n",
        "\t<li>Multi-word expressions → MWETokenizer with a small domain list</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3e47e202-2bf3-4552-b1fd-0d0b690fb7ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e47e202-2bf3-4552-b1fd-0d0b690fb7ff",
        "outputId": "2402e4a3-e50f-4922-d3da-fa96510a85fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "157d6dd6-da50-4e04-8736-9af5b963cc4a",
      "metadata": {
        "id": "157d6dd6-da50-4e04-8736-9af5b963cc4a"
      },
      "outputs": [],
      "source": [
        "# domain MWEs you can extend over time\n",
        "mwe_phrases = [\n",
        "    (\"follow\", \"up\"),\n",
        "    (\"action\", \"items\"),\n",
        "    (\"out\", \"of\", \"office\"),\n",
        "    (\"in\", \"person\"),\n",
        "    (\"on\", \"call\"),\n",
        "    (\"reach\", \"out\"),\n",
        "    (\"heads\", \"up\"),\n",
        "    (\"due\", \"diligence\"),\n",
        "    (\"next\", \"steps\"),\n",
        "    (\"as\", \"apap\"),  # example of a common typo cluster you might decide to keep together\n",
        "]\n",
        "\n",
        "mwe = MWETokenizer(mwe_phrases, separator=\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "69a29250-acce-4573-94df-793f2a1673e2",
      "metadata": {
        "id": "69a29250-acce-4573-94df-793f2a1673e2"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentences(text: str):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def tokenize_words_alpha(text: str):\n",
        "    \"\"\"Word tokenise, then keep only alphabetic tokens (emails often carry IDs, numbers, etc.).\"\"\"\n",
        "    toks = word_tokenize(text)\n",
        "    return [w for w in toks if re.fullmatch(r\"[A-Za-z_]+\", w)]\n",
        "\n",
        "def tokenize_words_with_mwe(text: str):\n",
        "    \"\"\"Apply MWE tokenizer after a basic split, then filter non-alphabetic (allows underscores).\"\"\"\n",
        "    toks = word_tokenize(text)\n",
        "    toks = mwe.tokenize(toks)\n",
        "    return [w for w in toks if re.fullmatch(r\"[A-Za-z_]+\", w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80x74mInv5iD",
      "metadata": {
        "id": "80x74mInv5iD"
      },
      "source": [
        "Let us create a small sample dataset to test our tokenization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "CPHKc1h6otXV",
      "metadata": {
        "id": "CPHKc1h6otXV"
      },
      "outputs": [],
      "source": [
        "sample_email_data = email_data[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9f52b249-18d1-4b60-95b4-a9d0de3a771b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f52b249-18d1-4b60-95b4-a9d0de3a771b",
        "outputId": "8391c45b-fe40-4d65-adc3-2f85924370f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== thread_id 1 | FW: Master Termination Log ===\n",
            "RAW  : \n",
            "\n",
            " -----Original Message-----\n",
            "From: =09Theriot, Kim S. =20\n",
            "Sent:=09Tuesday, January 29, 2002 1:23 PM\n",
            "To:=09Richardson, Stacey; Anderson, Diane; Gossett, Jeffrey\n",
            "NORM : to:=09richardson, stacey; anderson, diane; gossett, jeffrey c.; white, stac= ey w.; murphy, melissa; hall, d. todd; sweeney, kevin cc:=09aucoin, evelyn; baxter,\n",
            "#sents: 6  | #words(plain): 266  | #words(MWE): 266\n",
            "Top (plain): [('the', 4), ('termination', 4), ('no', 4), ('bruce', 3), ('as', 3), ('to', 2), ('murphy', 2), ('hall', 2)]\n",
            "Top (MWE)  : [('the', 4), ('termination', 4), ('no', 4), ('bruce', 3), ('as', 3), ('to', 2), ('murphy', 2), ('hall', 2)]\n",
            "MWE tokens captured: 0\n",
            "\n",
            "=== thread_id 1 | FW: Master Termination Log ===\n",
            "RAW  : \n",
            "\n",
            " -----Original Message-----\n",
            "From: =09Panus, Stephanie =20\n",
            "Sent:=09Thursday, January 31, 2002 12:08 PM\n",
            "To:=09Adams, Laurel; Albrecht, Kristin; Alonso, Tom; Aro\n",
            "NORM : to:=09adams, laurel; albrecht, kristin; alonso, tom; aronowitz, alan; baile= y, susan; balfour-flanagan, cyndie; baughman, edward; belden, tim; bishop, = serena\n",
            "#sents: 4  | #words(plain): 209  | #words(MWE): 209\n",
            "Top (plain): [('bruce', 3), ('robert', 3), ('january', 3), ('as', 3), ('to', 2), ('tom', 2), ('alan', 2), ('susan', 2)]\n",
            "Top (MWE)  : [('bruce', 3), ('robert', 3), ('january', 3), ('as', 3), ('to', 2), ('tom', 2), ('alan', 2), ('susan', 2)]\n",
            "MWE tokens captured: 0\n",
            "\n",
            "=== thread_id 1 | FW: Master Termination Log ===\n",
            "RAW  : Note to Stephanie Panus....\n",
            "\n",
            "Stephanie...please remove my name as well as Melissa Murphy's from the dist=\n",
            "ribution list below.\n",
            "\n",
            "Please add the following:\n",
            "\n",
            "Todd \n",
            "NORM : note to stephanie panus.... stephanie...please remove my name as well as melissa murphy's from the dist= ribution list below. please add the following: todd d. \n",
            "#sents: 5  | #words(plain): 230  | #words(MWE): 230\n",
            "Top (plain): [('as', 4), ('the', 4), ('stephanie', 3), ('tom', 3), ('bruce', 3), ('robert', 3), ('to', 2), ('panus', 2)]\n",
            "Top (MWE)  : [('as', 4), ('the', 4), ('stephanie', 3), ('tom', 3), ('bruce', 3), ('robert', 3), ('to', 2), ('panus', 2)]\n",
            "MWE tokens captured: 0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# assumes you have normalize_email_text(text) from Process 1\n",
        "def tokenisation_report(email_record):\n",
        "    raw = email_record.get(\"body\", \"\") or \"\"\n",
        "    norm = normalize_email_text(raw)\n",
        "\n",
        "    sents = tokenize_sentences(norm)\n",
        "    words_plain = tokenize_words_alpha(norm)\n",
        "    words_mwe  = tokenize_words_with_mwe(norm)\n",
        "\n",
        "    return {\n",
        "        \"thread_id\": email_record.get(\"thread_id\"),\n",
        "        \"subject\": email_record.get(\"subject\"),\n",
        "        \"raw_preview\": raw[:160],\n",
        "        \"norm_preview\": norm[:160],\n",
        "        \"n_sentences\": len(sents),\n",
        "        \"n_words_plain\": len(words_plain),\n",
        "        \"n_words_mwe\": len(words_mwe),\n",
        "        \"top_words_plain\": Counter(words_plain).most_common(8),\n",
        "        \"top_words_mwe\": Counter(words_mwe).most_common(8),\n",
        "        \"mwe_gain\": sum(1 for w in words_mwe if \"_\" in w),  # how many glued phrases we captured\n",
        "    }\n",
        "\n",
        "reports = [tokenisation_report(data) for data in sample_email_data]\n",
        "\n",
        "for r in reports[:3]:\n",
        "    print(f\"\\n=== thread_id {r['thread_id']} | {r['subject']} ===\")\n",
        "    print(\"RAW  :\", r[\"raw_preview\"])\n",
        "    print(\"NORM :\", r[\"norm_preview\"])\n",
        "    print(\"#sents:\", r[\"n_sentences\"], \" | #words(plain):\", r[\"n_words_plain\"], \" | #words(MWE):\", r[\"n_words_mwe\"])\n",
        "    print(\"Top (plain):\", r[\"top_words_plain\"])\n",
        "    print(\"Top (MWE)  :\", r[\"top_words_mwe\"])\n",
        "    print(\"MWE tokens captured:\", r[\"mwe_gain\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cmu31zejwDb7",
      "metadata": {
        "id": "Cmu31zejwDb7"
      },
      "source": [
        "From the above output, we can see the original text, it's normalised form and the corresponding tokenised version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r4DyymjdwQMo",
      "metadata": {
        "id": "r4DyymjdwQMo"
      },
      "source": [
        "### Stopword Removal\n",
        "\n",
        "**Stopwords** are common words (e.g., \"the\", \"is\", \"at\") that occur frequently but carry little semantic content. Removing them reduces dimensionality and focuses analysis on informative words.\n",
        "\n",
        "Email bodies often contain many function words. For advanced text processing, removing them highlights key terms and reduces the noisy words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "vhb5EpWli8Bo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhb5EpWli8Bo",
        "outputId": "68a6fc52-556b-484e-aa19-368ef6835ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stacey anderson diane gossett jeffrey white ey murphy melissa hall todd sweeney kevin cc evelyn baxter bryce wynne rita laurel alonso tom aronowitz alan bailey susan lanagan cyndie baughman edward belden tim bishop serena brackett ebbie bradford william browning mary nell bruce james bruce ichelle bruce robert buerkle jim calger christopher carrington lara considine keith cordova karen crandall sean cutsforth diamond russell dunton heather edison susan elafandi mo fischer mark flores nony fondren mark gorny vladimir gorte david gresham wayne hagelmann bjorn hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl little kelli llory chris mann kay mcginnis stephanie mcgrory robert mcmichael ed miller asset mktg moore janet moran tom murphy n murray julia nemec gerald ogden mary otto randy page jonalan ostlethwaite john prejean frank presto kevin puchot paul en dale richter brad richter jeff robison michael rohauer rosman stewart runswick stacy sacks edward scholtes diana ton sara simons paul swinney john thapar raj theriot kim jake thome stephen tricoli carl van hooser steve wente laura lson shona winfree woodland andrea yoder christian attached daily termination list january 25 well termination log incorporates terminations received ary 25 following previously master termination log en marked valid termination atlantic coast fibers transactions power agreement public utility district 1 chelan 01 connect energy services agreement ngl supply including financial ansactions referenced energy partners division ngl supply plains marketing plains marketing stephanie panus enron wholesale services ph fax\n",
            "laurel albrecht kristin alonso tom aronowitz alan susan cyndie baughman edward belden tim bishop serena boyd samantha brackett debbie bradford william mary nell bruce james bruce michelle bruce robert buerkle jim lger christopher carrington clara considine keith cordova karen crandall sean cutsforth diane diamond russell dunton heather son susan elafandi mo fischer mark flores nony fondren mark vladimir gorte david gresham wayne hagelmann bjorn hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl mallory chris mann kay mcginnis stephanie mcgrory robert mcmichael ed miller asset mktg moore janet tom murphy harlan murray julia nemec gerald ogden mary page alan postlethwaite john prejean frank presto kevin puchot paul rasmussen dale richardson stacey richter brad richter jeff robison michael rohauer tanya rosman stewart sacks edward scholtes sevitz robert shackleton sara simons paul swinney john thapar aj theriot kim thomas jake thome stephen tricoli carl van r steve wente laura wilson shona winfree woodland yoder christian attached daily lists january 29 january 30 well aster termination log incorporates terminations received january also prepetition mutual terminations added list identified nature default mutual n stephanie panus enron wholesale services ph fax\n",
            "note stephanie panus stephanie please remove name well melissa murphy ribution list please add following todd hall kevin sweeney rita wynne rebecca grace rhonda robinson kerri thomspon kristin albrecht tom chapman thanks kim theriot laurel albrecht kristin alonso tom aronowitz alan susan cyndie baughman edward belden tim bishop serena boyd samantha brackett debbie bradford william mary nell bruce james bruce michelle bruce robert buerkle jim lger christopher carrington clara chilkina elena considine keith cordova karen crandall sean cutsforth diane diamond russell ton heather edison susan elafandi mo fischer mark flores nony dren mark gorny vladimir gorte david gresham wayne hagelmann hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl mallory chris mann kay mcginnis phanie mcgrory robert mcmichael ed miller asset mktg janet moran tom murphy harlan murray julia nemec gerald mary page jonalan postlethwaite john prejean frank presto kevin puchot paul rasmussen dale richardson stacey richter brad r jeff robison michael rohauer tanya rosman stewart sacks scholtes diana sevitz robert shackleton sara simons paul john thapar raj theriot kim thomas jake thome stephen carl van hooser steve wente laura wilson shona winfree woodland andrea yoder christian attached daily list january 31 well master log incorporates terminations received january 31 stephanie panus enron wholesale services ph fax\n"
          ]
        }
      ],
      "source": [
        "# Stop word removal\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes common English stop words from a given string of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string to be processed.\n",
        "\n",
        "    Returns:\n",
        "        str: The text string after stop words have been removed.\n",
        "    \"\"\"\n",
        "    # 1. Get the set of English stop words\n",
        "    english_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # 2. Tokenize the input text\n",
        "    # We use word_tokenize for robust tokenization (handles punctuation better than simple split)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Filter out stop words\n",
        "    # Convert token to lowercase for comparison, as stop words are lowercase\n",
        "    filtered_tokens = [\n",
        "        word for word in tokens\n",
        "        if word.lower() not in english_stop_words and word.isalnum()\n",
        "    ]\n",
        "\n",
        "    # 4. Join the filtered tokens back into a single string\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "\n",
        "residue_text = []\n",
        "for data in sample_email_data:\n",
        "  residue_text.append(remove_stopwords(normalize_email_text(data['body'])))\n",
        "\n",
        "for r in residue_text[:3]:\n",
        "  print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I4m8jwv_w89d",
      "metadata": {
        "id": "I4m8jwv_w89d"
      },
      "source": [
        "### Morphological analysis\n",
        "\n",
        "**Morphology** studies word forms. Two common techniques:\n",
        "\n",
        "- **Stemming**: chop suffixes to get the crude root (e.g., \"studies\" -> \"studi\").\n",
        "- **Lemmatisation**: reduce to dictionary form using linguistic rules\n",
        "  (e.g., \"studies\" -> \"study\").\n",
        "\n",
        "Emails use different inflections of the same word (\"terminate\", \"terminated\", \"termination\"). Coverting them to their root form helps group them, improving vocabulary consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "snPRkMFwqmLP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snPRkMFwqmLP",
        "outputId": "366dfd5f-c3f3-4d0e-fb0f-bbcf244161c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------\n",
            "\n",
            "Original Text: stacey anderson diane gossett jeffrey white ey murphy melissa hall todd sweeney kevin cc evelyn baxter bryce wynne rita laurel alonso tom aronowitz alan bailey susan lanagan cyndie baughman edward belden tim bishop serena brackett ebbie bradford william browning mary nell bruce james bruce ichelle bruce robert buerkle jim calger christopher carrington lara considine keith cordova karen crandall sean cutsforth diamond russell dunton heather edison susan elafandi mo fischer mark flores nony fondren mark gorny vladimir gorte david gresham wayne hagelmann bjorn hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl little kelli llory chris mann kay mcginnis stephanie mcgrory robert mcmichael ed miller asset mktg moore janet moran tom murphy n murray julia nemec gerald ogden mary otto randy page jonalan ostlethwaite john prejean frank presto kevin puchot paul en dale richter brad richter jeff robison michael rohauer rosman stewart runswick stacy sacks edward scholtes diana ton sara simons paul swinney john thapar raj theriot kim jake thome stephen tricoli carl van hooser steve wente laura lson shona winfree woodland andrea yoder christian attached daily termination list january 25 well termination log incorporates terminations received ary 25 following previously master termination log en marked valid termination atlantic coast fibers transactions power agreement public utility district 1 chelan 01 connect energy services agreement ngl supply including financial ansactions referenced energy partners division ngl supply plains marketing plains marketing stephanie panus enron wholesale services ph fax\n",
            "Stemming Result: [('stacey', 'stacey'), ('anderson', 'anderson'), ('diane', 'dian'), ('gossett', 'gossett'), ('jeffrey', 'jeffrey'), ('white', 'white'), ('ey', 'ey'), ('murphy', 'murphi'), ('melissa', 'melissa'), ('hall', 'hall'), ('todd', 'todd'), ('sweeney', 'sweeney'), ('kevin', 'kevin'), ('cc', 'cc'), ('evelyn', 'evelyn'), ('baxter', 'baxter'), ('bryce', 'bryce'), ('wynne', 'wynn'), ('rita', 'rita'), ('laurel', 'laurel'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('bailey', 'bailey'), ('susan', 'susan'), ('lanagan', 'lanagan'), ('cyndie', 'cyndi'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('brackett', 'brackett'), ('ebbie', 'ebbi'), ('bradford', 'bradford'), ('william', 'william'), ('browning', 'brown'), ('mary', 'mari'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jame'), ('bruce', 'bruce'), ('ichelle', 'ichel'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkl'), ('jim', 'jim'), ('calger', 'calger'), ('christopher', 'christoph'), ('carrington', 'carrington'), ('lara', 'lara'), ('considine', 'considin'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandal'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diamond', 'diamond'), ('russell', 'russel'), ('dunton', 'dunton'), ('heather', 'heather'), ('edison', 'edison'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flore'), ('nony', 'noni'), ('fondren', 'fondren'), ('mark', 'mark'), ('gorny', 'gorni'), ('vladimir', 'vladimir'), ('gorte', 'gort'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayn'), ('hagelmann', 'hagelmann'), ('bjorn', 'bjorn'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'hark'), ('cynthia', 'cynthia'), ('hendry', 'hendri'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohan'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('little', 'littl'), ('kelli', 'kelli'), ('llory', 'llori'), ('chris', 'chri'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginni'), ('stephanie', 'stephani'), ('mcgrory', 'mcgrori'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('moore', 'moor'), ('janet', 'janet'), ('moran', 'moran'), ('tom', 'tom'), ('murphy', 'murphi'), ('n', 'n'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('ogden', 'ogden'), ('mary', 'mari'), ('otto', 'otto'), ('randy', 'randi'), ('page', 'page'), ('jonalan', 'jonalan'), ('ostlethwaite', 'ostlethwait'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('en', 'en'), ('dale', 'dale'), ('richter', 'richter'), ('brad', 'brad'), ('richter', 'richter'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('runswick', 'runswick'), ('stacy', 'staci'), ('sacks', 'sack'), ('edward', 'edward'), ('scholtes', 'scholt'), ('diana', 'diana'), ('ton', 'ton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('swinney', 'swinney'), ('john', 'john'), ('thapar', 'thapar'), ('raj', 'raj'), ('theriot', 'theriot'), ('kim', 'kim'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('tricoli', 'tricoli'), ('carl', 'carl'), ('van', 'van'), ('hooser', 'hooser'), ('steve', 'steve'), ('wente', 'went'), ('laura', 'laura'), ('lson', 'lson'), ('shona', 'shona'), ('winfree', 'winfre'), ('woodland', 'woodland'), ('andrea', 'andrea'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daili'), ('termination', 'termin'), ('list', 'list'), ('january', 'januari'), ('25', '25'), ('well', 'well'), ('termination', 'termin'), ('log', 'log'), ('incorporates', 'incorpor'), ('terminations', 'termin'), ('received', 'receiv'), ('ary', 'ari'), ('25', '25'), ('following', 'follow'), ('previously', 'previous'), ('master', 'master'), ('termination', 'termin'), ('log', 'log'), ('en', 'en'), ('marked', 'mark'), ('valid', 'valid'), ('termination', 'termin'), ('atlantic', 'atlant'), ('coast', 'coast'), ('fibers', 'fiber'), ('transactions', 'transact'), ('power', 'power'), ('agreement', 'agreement'), ('public', 'public'), ('utility', 'util'), ('district', 'district'), ('1', '1'), ('chelan', 'chelan'), ('01', '01'), ('connect', 'connect'), ('energy', 'energi'), ('services', 'servic'), ('agreement', 'agreement'), ('ngl', 'ngl'), ('supply', 'suppli'), ('including', 'includ'), ('financial', 'financi'), ('ansactions', 'ansact'), ('referenced', 'referenc'), ('energy', 'energi'), ('partners', 'partner'), ('division', 'divis'), ('ngl', 'ngl'), ('supply', 'suppli'), ('plains', 'plain'), ('marketing', 'market'), ('plains', 'plain'), ('marketing', 'market'), ('stephanie', 'stephani'), ('panus', 'panu'), ('enron', 'enron'), ('wholesale', 'wholesal'), ('services', 'servic'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "Lemmatization Result: [('stacey', 'stacey'), ('anderson', 'anderson'), ('diane', 'diane'), ('gossett', 'gossett'), ('jeffrey', 'jeffrey'), ('white', 'white'), ('ey', 'ey'), ('murphy', 'murphy'), ('melissa', 'melissa'), ('hall', 'hall'), ('todd', 'todd'), ('sweeney', 'sweeney'), ('kevin', 'kevin'), ('cc', 'cc'), ('evelyn', 'evelyn'), ('baxter', 'baxter'), ('bryce', 'bryce'), ('wynne', 'wynne'), ('rita', 'rita'), ('laurel', 'laurel'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('bailey', 'bailey'), ('susan', 'susan'), ('lanagan', 'lanagan'), ('cyndie', 'cyndie'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('brackett', 'brackett'), ('ebbie', 'ebbie'), ('bradford', 'bradford'), ('william', 'william'), ('browning', 'brown'), ('mary', 'mary'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jam'), ('bruce', 'bruce'), ('ichelle', 'ichelle'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkle'), ('jim', 'jim'), ('calger', 'calger'), ('christopher', 'christopher'), ('carrington', 'carrington'), ('lara', 'lara'), ('considine', 'considine'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandall'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diamond', 'diamond'), ('russell', 'russell'), ('dunton', 'dunton'), ('heather', 'heather'), ('edison', 'edison'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flores'), ('nony', 'nony'), ('fondren', 'fondren'), ('mark', 'mark'), ('gorny', 'gorny'), ('vladimir', 'vladimir'), ('gorte', 'gorte'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayne'), ('hagelmann', 'hagelmann'), ('bjorn', 'bjorn'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'harkness'), ('cynthia', 'cynthia'), ('hendry', 'hendry'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohane'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('little', 'little'), ('kelli', 'kelli'), ('llory', 'llory'), ('chris', 'chris'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginnis'), ('stephanie', 'stephanie'), ('mcgrory', 'mcgrory'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('moore', 'moore'), ('janet', 'janet'), ('moran', 'moran'), ('tom', 'tom'), ('murphy', 'murphy'), ('n', 'n'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('ogden', 'ogden'), ('mary', 'mary'), ('otto', 'otto'), ('randy', 'randy'), ('page', 'page'), ('jonalan', 'jonalan'), ('ostlethwaite', 'ostlethwaite'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('en', 'en'), ('dale', 'dale'), ('richter', 'richter'), ('brad', 'brad'), ('richter', 'richter'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('runswick', 'runswick'), ('stacy', 'stacy'), ('sacks', 'sack'), ('edward', 'edward'), ('scholtes', 'scholtes'), ('diana', 'diana'), ('ton', 'ton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('swinney', 'swinney'), ('john', 'john'), ('thapar', 'thapar'), ('raj', 'raj'), ('theriot', 'theriot'), ('kim', 'kim'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('tricoli', 'tricoli'), ('carl', 'carl'), ('van', 'van'), ('hooser', 'hooser'), ('steve', 'steve'), ('wente', 'wente'), ('laura', 'laura'), ('lson', 'lson'), ('shona', 'shona'), ('winfree', 'winfree'), ('woodland', 'woodland'), ('andrea', 'andrea'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daily'), ('termination', 'termination'), ('list', 'list'), ('january', 'january'), ('25', '25'), ('well', 'well'), ('termination', 'termination'), ('log', 'log'), ('incorporates', 'incorporate'), ('terminations', 'termination'), ('received', 'receive'), ('ary', 'ary'), ('25', '25'), ('following', 'follow'), ('previously', 'previously'), ('master', 'master'), ('termination', 'termination'), ('log', 'log'), ('en', 'en'), ('marked', 'mark'), ('valid', 'valid'), ('termination', 'termination'), ('atlantic', 'atlantic'), ('coast', 'coast'), ('fibers', 'fiber'), ('transactions', 'transaction'), ('power', 'power'), ('agreement', 'agreement'), ('public', 'public'), ('utility', 'utility'), ('district', 'district'), ('1', '1'), ('chelan', 'chelan'), ('01', '01'), ('connect', 'connect'), ('energy', 'energy'), ('services', 'service'), ('agreement', 'agreement'), ('ngl', 'ngl'), ('supply', 'supply'), ('including', 'include'), ('financial', 'financial'), ('ansactions', 'ansactions'), ('referenced', 'reference'), ('energy', 'energy'), ('partners', 'partner'), ('division', 'division'), ('ngl', 'ngl'), ('supply', 'supply'), ('plains', 'plain'), ('marketing', 'market'), ('plains', 'plain'), ('marketing', 'market'), ('stephanie', 'stephanie'), ('panus', 'panus'), ('enron', 'enron'), ('wholesale', 'wholesale'), ('services', 'service'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "\n",
            "--------------------------\n",
            "\n",
            "\n",
            "--------------------------\n",
            "\n",
            "Original Text: laurel albrecht kristin alonso tom aronowitz alan susan cyndie baughman edward belden tim bishop serena boyd samantha brackett debbie bradford william mary nell bruce james bruce michelle bruce robert buerkle jim lger christopher carrington clara considine keith cordova karen crandall sean cutsforth diane diamond russell dunton heather son susan elafandi mo fischer mark flores nony fondren mark vladimir gorte david gresham wayne hagelmann bjorn hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl mallory chris mann kay mcginnis stephanie mcgrory robert mcmichael ed miller asset mktg moore janet tom murphy harlan murray julia nemec gerald ogden mary page alan postlethwaite john prejean frank presto kevin puchot paul rasmussen dale richardson stacey richter brad richter jeff robison michael rohauer tanya rosman stewart sacks edward scholtes sevitz robert shackleton sara simons paul swinney john thapar aj theriot kim thomas jake thome stephen tricoli carl van r steve wente laura wilson shona winfree woodland yoder christian attached daily lists january 29 january 30 well aster termination log incorporates terminations received january also prepetition mutual terminations added list identified nature default mutual n stephanie panus enron wholesale services ph fax\n",
            "Stemming Result: [('laurel', 'laurel'), ('albrecht', 'albrecht'), ('kristin', 'kristin'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('susan', 'susan'), ('cyndie', 'cyndi'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('boyd', 'boyd'), ('samantha', 'samantha'), ('brackett', 'brackett'), ('debbie', 'debbi'), ('bradford', 'bradford'), ('william', 'william'), ('mary', 'mari'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jame'), ('bruce', 'bruce'), ('michelle', 'michel'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkl'), ('jim', 'jim'), ('lger', 'lger'), ('christopher', 'christoph'), ('carrington', 'carrington'), ('clara', 'clara'), ('considine', 'considin'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandal'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diane', 'dian'), ('diamond', 'diamond'), ('russell', 'russel'), ('dunton', 'dunton'), ('heather', 'heather'), ('son', 'son'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flore'), ('nony', 'noni'), ('fondren', 'fondren'), ('mark', 'mark'), ('vladimir', 'vladimir'), ('gorte', 'gort'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayn'), ('hagelmann', 'hagelmann'), ('bjorn', 'bjorn'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'hark'), ('cynthia', 'cynthia'), ('hendry', 'hendri'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohan'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('mallory', 'mallori'), ('chris', 'chri'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginni'), ('stephanie', 'stephani'), ('mcgrory', 'mcgrori'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('moore', 'moor'), ('janet', 'janet'), ('tom', 'tom'), ('murphy', 'murphi'), ('harlan', 'harlan'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('ogden', 'ogden'), ('mary', 'mari'), ('page', 'page'), ('alan', 'alan'), ('postlethwaite', 'postlethwait'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('rasmussen', 'rasmussen'), ('dale', 'dale'), ('richardson', 'richardson'), ('stacey', 'stacey'), ('richter', 'richter'), ('brad', 'brad'), ('richter', 'richter'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('tanya', 'tanya'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('sacks', 'sack'), ('edward', 'edward'), ('scholtes', 'scholt'), ('sevitz', 'sevitz'), ('robert', 'robert'), ('shackleton', 'shackleton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('swinney', 'swinney'), ('john', 'john'), ('thapar', 'thapar'), ('aj', 'aj'), ('theriot', 'theriot'), ('kim', 'kim'), ('thomas', 'thoma'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('tricoli', 'tricoli'), ('carl', 'carl'), ('van', 'van'), ('r', 'r'), ('steve', 'steve'), ('wente', 'went'), ('laura', 'laura'), ('wilson', 'wilson'), ('shona', 'shona'), ('winfree', 'winfre'), ('woodland', 'woodland'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daili'), ('lists', 'list'), ('january', 'januari'), ('29', '29'), ('january', 'januari'), ('30', '30'), ('well', 'well'), ('aster', 'aster'), ('termination', 'termin'), ('log', 'log'), ('incorporates', 'incorpor'), ('terminations', 'termin'), ('received', 'receiv'), ('january', 'januari'), ('also', 'also'), ('prepetition', 'prepetit'), ('mutual', 'mutual'), ('terminations', 'termin'), ('added', 'ad'), ('list', 'list'), ('identified', 'identifi'), ('nature', 'natur'), ('default', 'default'), ('mutual', 'mutual'), ('n', 'n'), ('stephanie', 'stephani'), ('panus', 'panu'), ('enron', 'enron'), ('wholesale', 'wholesal'), ('services', 'servic'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "Lemmatization Result: [('laurel', 'laurel'), ('albrecht', 'albrecht'), ('kristin', 'kristin'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('susan', 'susan'), ('cyndie', 'cyndie'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('boyd', 'boyd'), ('samantha', 'samantha'), ('brackett', 'brackett'), ('debbie', 'debbie'), ('bradford', 'bradford'), ('william', 'william'), ('mary', 'mary'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jam'), ('bruce', 'bruce'), ('michelle', 'michelle'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkle'), ('jim', 'jim'), ('lger', 'lger'), ('christopher', 'christopher'), ('carrington', 'carrington'), ('clara', 'clara'), ('considine', 'considine'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandall'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diane', 'diane'), ('diamond', 'diamond'), ('russell', 'russell'), ('dunton', 'dunton'), ('heather', 'heather'), ('son', 'son'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flores'), ('nony', 'nony'), ('fondren', 'fondren'), ('mark', 'mark'), ('vladimir', 'vladimir'), ('gorte', 'gorte'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayne'), ('hagelmann', 'hagelmann'), ('bjorn', 'bjorn'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'harkness'), ('cynthia', 'cynthia'), ('hendry', 'hendry'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohane'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('mallory', 'mallory'), ('chris', 'chris'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginnis'), ('stephanie', 'stephanie'), ('mcgrory', 'mcgrory'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('moore', 'moore'), ('janet', 'janet'), ('tom', 'tom'), ('murphy', 'murphy'), ('harlan', 'harlan'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('ogden', 'ogden'), ('mary', 'mary'), ('page', 'page'), ('alan', 'alan'), ('postlethwaite', 'postlethwaite'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('rasmussen', 'rasmussen'), ('dale', 'dale'), ('richardson', 'richardson'), ('stacey', 'stacey'), ('richter', 'richter'), ('brad', 'brad'), ('richter', 'richter'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('tanya', 'tanya'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('sacks', 'sack'), ('edward', 'edward'), ('scholtes', 'scholtes'), ('sevitz', 'sevitz'), ('robert', 'robert'), ('shackleton', 'shackleton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('swinney', 'swinney'), ('john', 'john'), ('thapar', 'thapar'), ('aj', 'aj'), ('theriot', 'theriot'), ('kim', 'kim'), ('thomas', 'thomas'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('tricoli', 'tricoli'), ('carl', 'carl'), ('van', 'van'), ('r', 'r'), ('steve', 'steve'), ('wente', 'wente'), ('laura', 'laura'), ('wilson', 'wilson'), ('shona', 'shona'), ('winfree', 'winfree'), ('woodland', 'woodland'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daily'), ('lists', 'list'), ('january', 'january'), ('29', '29'), ('january', 'january'), ('30', '30'), ('well', 'well'), ('aster', 'aster'), ('termination', 'termination'), ('log', 'log'), ('incorporates', 'incorporate'), ('terminations', 'termination'), ('received', 'receive'), ('january', 'january'), ('also', 'also'), ('prepetition', 'prepetition'), ('mutual', 'mutual'), ('terminations', 'termination'), ('added', 'add'), ('list', 'list'), ('identified', 'identify'), ('nature', 'nature'), ('default', 'default'), ('mutual', 'mutual'), ('n', 'n'), ('stephanie', 'stephanie'), ('panus', 'panus'), ('enron', 'enron'), ('wholesale', 'wholesale'), ('services', 'service'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "\n",
            "--------------------------\n",
            "\n",
            "\n",
            "--------------------------\n",
            "\n",
            "Original Text: note stephanie panus stephanie please remove name well melissa murphy ribution list please add following todd hall kevin sweeney rita wynne rebecca grace rhonda robinson kerri thomspon kristin albrecht tom chapman thanks kim theriot laurel albrecht kristin alonso tom aronowitz alan susan cyndie baughman edward belden tim bishop serena boyd samantha brackett debbie bradford william mary nell bruce james bruce michelle bruce robert buerkle jim lger christopher carrington clara chilkina elena considine keith cordova karen crandall sean cutsforth diane diamond russell ton heather edison susan elafandi mo fischer mark flores nony dren mark gorny vladimir gorte david gresham wayne hagelmann hall steve legal harkness cynthia hendry brent johnston greg keohane peter lindeman cheryl mallory chris mann kay mcginnis phanie mcgrory robert mcmichael ed miller asset mktg janet moran tom murphy harlan murray julia nemec gerald mary page jonalan postlethwaite john prejean frank presto kevin puchot paul rasmussen dale richardson stacey richter brad r jeff robison michael rohauer tanya rosman stewart sacks scholtes diana sevitz robert shackleton sara simons paul john thapar raj theriot kim thomas jake thome stephen carl van hooser steve wente laura wilson shona winfree woodland andrea yoder christian attached daily list january 31 well master log incorporates terminations received january 31 stephanie panus enron wholesale services ph fax\n",
            "Stemming Result: [('note', 'note'), ('stephanie', 'stephani'), ('panus', 'panu'), ('stephanie', 'stephani'), ('please', 'pleas'), ('remove', 'remov'), ('name', 'name'), ('well', 'well'), ('melissa', 'melissa'), ('murphy', 'murphi'), ('ribution', 'ribut'), ('list', 'list'), ('please', 'pleas'), ('add', 'add'), ('following', 'follow'), ('todd', 'todd'), ('hall', 'hall'), ('kevin', 'kevin'), ('sweeney', 'sweeney'), ('rita', 'rita'), ('wynne', 'wynn'), ('rebecca', 'rebecca'), ('grace', 'grace'), ('rhonda', 'rhonda'), ('robinson', 'robinson'), ('kerri', 'kerri'), ('thomspon', 'thomspon'), ('kristin', 'kristin'), ('albrecht', 'albrecht'), ('tom', 'tom'), ('chapman', 'chapman'), ('thanks', 'thank'), ('kim', 'kim'), ('theriot', 'theriot'), ('laurel', 'laurel'), ('albrecht', 'albrecht'), ('kristin', 'kristin'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('susan', 'susan'), ('cyndie', 'cyndi'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('boyd', 'boyd'), ('samantha', 'samantha'), ('brackett', 'brackett'), ('debbie', 'debbi'), ('bradford', 'bradford'), ('william', 'william'), ('mary', 'mari'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jame'), ('bruce', 'bruce'), ('michelle', 'michel'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkl'), ('jim', 'jim'), ('lger', 'lger'), ('christopher', 'christoph'), ('carrington', 'carrington'), ('clara', 'clara'), ('chilkina', 'chilkina'), ('elena', 'elena'), ('considine', 'considin'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandal'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diane', 'dian'), ('diamond', 'diamond'), ('russell', 'russel'), ('ton', 'ton'), ('heather', 'heather'), ('edison', 'edison'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flore'), ('nony', 'noni'), ('dren', 'dren'), ('mark', 'mark'), ('gorny', 'gorni'), ('vladimir', 'vladimir'), ('gorte', 'gort'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayn'), ('hagelmann', 'hagelmann'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'hark'), ('cynthia', 'cynthia'), ('hendry', 'hendri'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohan'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('mallory', 'mallori'), ('chris', 'chri'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginni'), ('phanie', 'phani'), ('mcgrory', 'mcgrori'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('janet', 'janet'), ('moran', 'moran'), ('tom', 'tom'), ('murphy', 'murphi'), ('harlan', 'harlan'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('mary', 'mari'), ('page', 'page'), ('jonalan', 'jonalan'), ('postlethwaite', 'postlethwait'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('rasmussen', 'rasmussen'), ('dale', 'dale'), ('richardson', 'richardson'), ('stacey', 'stacey'), ('richter', 'richter'), ('brad', 'brad'), ('r', 'r'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('tanya', 'tanya'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('sacks', 'sack'), ('scholtes', 'scholt'), ('diana', 'diana'), ('sevitz', 'sevitz'), ('robert', 'robert'), ('shackleton', 'shackleton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('john', 'john'), ('thapar', 'thapar'), ('raj', 'raj'), ('theriot', 'theriot'), ('kim', 'kim'), ('thomas', 'thoma'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('carl', 'carl'), ('van', 'van'), ('hooser', 'hooser'), ('steve', 'steve'), ('wente', 'went'), ('laura', 'laura'), ('wilson', 'wilson'), ('shona', 'shona'), ('winfree', 'winfre'), ('woodland', 'woodland'), ('andrea', 'andrea'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daili'), ('list', 'list'), ('january', 'januari'), ('31', '31'), ('well', 'well'), ('master', 'master'), ('log', 'log'), ('incorporates', 'incorpor'), ('terminations', 'termin'), ('received', 'receiv'), ('january', 'januari'), ('31', '31'), ('stephanie', 'stephani'), ('panus', 'panu'), ('enron', 'enron'), ('wholesale', 'wholesal'), ('services', 'servic'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "Lemmatization Result: [('note', 'note'), ('stephanie', 'stephanie'), ('panus', 'panus'), ('stephanie', 'stephanie'), ('please', 'please'), ('remove', 'remove'), ('name', 'name'), ('well', 'well'), ('melissa', 'melissa'), ('murphy', 'murphy'), ('ribution', 'ribution'), ('list', 'list'), ('please', 'please'), ('add', 'add'), ('following', 'follow'), ('todd', 'todd'), ('hall', 'hall'), ('kevin', 'kevin'), ('sweeney', 'sweeney'), ('rita', 'rita'), ('wynne', 'wynne'), ('rebecca', 'rebecca'), ('grace', 'grace'), ('rhonda', 'rhonda'), ('robinson', 'robinson'), ('kerri', 'kerri'), ('thomspon', 'thomspon'), ('kristin', 'kristin'), ('albrecht', 'albrecht'), ('tom', 'tom'), ('chapman', 'chapman'), ('thanks', 'thank'), ('kim', 'kim'), ('theriot', 'theriot'), ('laurel', 'laurel'), ('albrecht', 'albrecht'), ('kristin', 'kristin'), ('alonso', 'alonso'), ('tom', 'tom'), ('aronowitz', 'aronowitz'), ('alan', 'alan'), ('susan', 'susan'), ('cyndie', 'cyndie'), ('baughman', 'baughman'), ('edward', 'edward'), ('belden', 'belden'), ('tim', 'tim'), ('bishop', 'bishop'), ('serena', 'serena'), ('boyd', 'boyd'), ('samantha', 'samantha'), ('brackett', 'brackett'), ('debbie', 'debbie'), ('bradford', 'bradford'), ('william', 'william'), ('mary', 'mary'), ('nell', 'nell'), ('bruce', 'bruce'), ('james', 'jam'), ('bruce', 'bruce'), ('michelle', 'michelle'), ('bruce', 'bruce'), ('robert', 'robert'), ('buerkle', 'buerkle'), ('jim', 'jim'), ('lger', 'lger'), ('christopher', 'christopher'), ('carrington', 'carrington'), ('clara', 'clara'), ('chilkina', 'chilkina'), ('elena', 'elena'), ('considine', 'considine'), ('keith', 'keith'), ('cordova', 'cordova'), ('karen', 'karen'), ('crandall', 'crandall'), ('sean', 'sean'), ('cutsforth', 'cutsforth'), ('diane', 'diane'), ('diamond', 'diamond'), ('russell', 'russell'), ('ton', 'ton'), ('heather', 'heather'), ('edison', 'edison'), ('susan', 'susan'), ('elafandi', 'elafandi'), ('mo', 'mo'), ('fischer', 'fischer'), ('mark', 'mark'), ('flores', 'flores'), ('nony', 'nony'), ('dren', 'dren'), ('mark', 'mark'), ('gorny', 'gorny'), ('vladimir', 'vladimir'), ('gorte', 'gorte'), ('david', 'david'), ('gresham', 'gresham'), ('wayne', 'wayne'), ('hagelmann', 'hagelmann'), ('hall', 'hall'), ('steve', 'steve'), ('legal', 'legal'), ('harkness', 'harkness'), ('cynthia', 'cynthia'), ('hendry', 'hendry'), ('brent', 'brent'), ('johnston', 'johnston'), ('greg', 'greg'), ('keohane', 'keohane'), ('peter', 'peter'), ('lindeman', 'lindeman'), ('cheryl', 'cheryl'), ('mallory', 'mallory'), ('chris', 'chris'), ('mann', 'mann'), ('kay', 'kay'), ('mcginnis', 'mcginnis'), ('phanie', 'phanie'), ('mcgrory', 'mcgrory'), ('robert', 'robert'), ('mcmichael', 'mcmichael'), ('ed', 'ed'), ('miller', 'miller'), ('asset', 'asset'), ('mktg', 'mktg'), ('janet', 'janet'), ('moran', 'moran'), ('tom', 'tom'), ('murphy', 'murphy'), ('harlan', 'harlan'), ('murray', 'murray'), ('julia', 'julia'), ('nemec', 'nemec'), ('gerald', 'gerald'), ('mary', 'mary'), ('page', 'page'), ('jonalan', 'jonalan'), ('postlethwaite', 'postlethwaite'), ('john', 'john'), ('prejean', 'prejean'), ('frank', 'frank'), ('presto', 'presto'), ('kevin', 'kevin'), ('puchot', 'puchot'), ('paul', 'paul'), ('rasmussen', 'rasmussen'), ('dale', 'dale'), ('richardson', 'richardson'), ('stacey', 'stacey'), ('richter', 'richter'), ('brad', 'brad'), ('r', 'r'), ('jeff', 'jeff'), ('robison', 'robison'), ('michael', 'michael'), ('rohauer', 'rohauer'), ('tanya', 'tanya'), ('rosman', 'rosman'), ('stewart', 'stewart'), ('sacks', 'sack'), ('scholtes', 'scholtes'), ('diana', 'diana'), ('sevitz', 'sevitz'), ('robert', 'robert'), ('shackleton', 'shackleton'), ('sara', 'sara'), ('simons', 'simon'), ('paul', 'paul'), ('john', 'john'), ('thapar', 'thapar'), ('raj', 'raj'), ('theriot', 'theriot'), ('kim', 'kim'), ('thomas', 'thomas'), ('jake', 'jake'), ('thome', 'thome'), ('stephen', 'stephen'), ('carl', 'carl'), ('van', 'van'), ('hooser', 'hooser'), ('steve', 'steve'), ('wente', 'wente'), ('laura', 'laura'), ('wilson', 'wilson'), ('shona', 'shona'), ('winfree', 'winfree'), ('woodland', 'woodland'), ('andrea', 'andrea'), ('yoder', 'yoder'), ('christian', 'christian'), ('attached', 'attach'), ('daily', 'daily'), ('list', 'list'), ('january', 'january'), ('31', '31'), ('well', 'well'), ('master', 'master'), ('log', 'log'), ('incorporates', 'incorporate'), ('terminations', 'termination'), ('received', 'receive'), ('january', 'january'), ('31', '31'), ('stephanie', 'stephanie'), ('panus', 'panus'), ('enron', 'enron'), ('wholesale', 'wholesale'), ('services', 'service'), ('ph', 'ph'), ('fax', 'fax')]\n",
            "\n",
            "--------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "def morphological_analysis(text: str) -> Dict[str, List[Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Performs morphological analysis (stemming and lemmatisation) on the input text.\n",
        "\n",
        "    Stemming reduces words to their root/stem (e.g., 'studies' -> 'studi').\n",
        "    Lemmatisation reduces words to their base dictionary form (e.g., 'studies' -> 'study').\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[Tuple[str, str]]]: A dictionary containing two lists of\n",
        "        (original word, reduced form) tuples: one for stemming and one for lemmatisation.\n",
        "    \"\"\"\n",
        "    # Initialize stemmer and lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Perform Stemming\n",
        "    stem_results = []\n",
        "    for token in tokens:\n",
        "        # Only process alphanumeric tokens\n",
        "        if token.isalnum():\n",
        "            stemmed_word = stemmer.stem(token)\n",
        "            stem_results.append((token, stemmed_word))\n",
        "\n",
        "    # Perform Lemmatisation (using default POS='n' (noun) for simplicity)\n",
        "    lemma_results = []\n",
        "    for token in tokens:\n",
        "        if token.isalnum():\n",
        "            # Lemmatisation is more accurate when POS tag is provided, but we use 'n'\n",
        "            # (noun) or 'v' (verb) by default if not provided. We try 'v' for better results on verbs.\n",
        "            lemmatized_word = lemmatizer.lemmatize(token, pos='v')\n",
        "            if lemmatized_word == token:\n",
        "                # If verb lemmatisation failed, try noun lemmatisation\n",
        "                lemmatized_word = lemmatizer.lemmatize(token, pos='n')\n",
        "\n",
        "            lemma_results.append((token, lemmatized_word))\n",
        "\n",
        "    return {\n",
        "        \"stemming\": stem_results,\n",
        "        \"lemmatization\": lemma_results\n",
        "    }\n",
        "\n",
        "# --- Example Usage ---\n",
        "for text in residue_text:\n",
        "  analysis_output = morphological_analysis(text)\n",
        "  print(f\"\\n--------------------------\\n\")\n",
        "  print(f\"Original Text: {text}\")\n",
        "  print(f\"Stemming Result: {analysis_output['stemming']}\")\n",
        "  print(f\"Lemmatization Result: {analysis_output['lemmatization']}\")\n",
        "  print(f\"\\n--------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HSj_tR1RxU7-",
      "metadata": {
        "id": "HSj_tR1RxU7-"
      },
      "source": [
        "As you can see from the last output, although both stemming and lemmatisation tries to convert a given word into its root form, there is a subtle difference: the output of lemmatisation is always a valid dictionary word, while for stemming that might not be the case. For example, the stemmed output of the word 'pleased' is 'pleas', which is not a valid dictionary word. Whereas, the lemmatised version of 'pleased' is 'please'."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05A8FTSxx-4U",
      "metadata": {
        "id": "05A8FTSxx-4U"
      },
      "source": [
        "To convert a non-dictionary word into a dictionary word, we can further perform a spelling correction, discussed next."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6xijiMnSxiF0",
      "metadata": {
        "id": "6xijiMnSxiF0"
      },
      "source": [
        "### Spell Correction (Noisy Channel Model)\n",
        "\n",
        "Emails and other text content contain typos (\"teh\" for \"the\", \"langauge\" for \"language\").\n",
        "Correcting them ensures vocabulary consistency and better downstream NLP.\n",
        "\n",
        "Spelling correction in NLP can be done by the following approaches:\n",
        "- **Edit distance (Levenshtein distance)**: minimal operations to turn one\n",
        "  word into another (e.g., \"speling\" -> \"spelling\" distance=1).\n",
        "- **Noisy channel model**: given a possibly misspelled word, pick the candidate\n",
        "  that maximises P(correct_word) * P(observed|correct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Cj5tpgcVjxaJ",
      "metadata": {
        "id": "Cj5tpgcVjxaJ"
      },
      "outputs": [],
      "source": [
        "# Try NLTK edit_distance, otherwise fallback\n",
        "from nltk.metrics import edit_distance as nltk_edit_distance\n",
        "import math\n",
        "\n",
        "# Fallback Levenshtein distance\n",
        "def levenshtein(s1, s2):\n",
        "    if s1 == s2:\n",
        "        return 0\n",
        "    len1, len2 = len(s1), len(s2)\n",
        "    if len1 == 0: return len2\n",
        "    if len2 == 0: return len1\n",
        "    prev_row = list(range(len2 + 1))\n",
        "    for i, c1 in enumerate(s1, start=1):\n",
        "        cur_row = [i] + [0] * len2\n",
        "        for j, c2 in enumerate(s2, start=1):\n",
        "            insert_cost = cur_row[j-1] + 1\n",
        "            delete_cost = prev_row[j] + 1\n",
        "            replace_cost = prev_row[j-1] + (0 if c1 == c2 else 1)\n",
        "            cur_row[j] = min(insert_cost, delete_cost, replace_cost)\n",
        "        prev_row = cur_row\n",
        "    return prev_row[-1]\n",
        "\n",
        "def edit_distance(a, b):\n",
        "    \"\"\"Use NLTK’s edit_distance if available, else fallback.\"\"\"\n",
        "    if nltk_edit_distance:\n",
        "        return nltk_edit_distance(a, b)\n",
        "    return levenshtein(a, b)\n",
        "\n",
        "def edit_distance_wo_fallback(a, b):\n",
        "    return nltk_edit_distance(a, b)\n",
        "\n",
        "# ---- Toy corpus with frequency counts ----\n",
        "toy_corpus = [\n",
        "    \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\",\n",
        "    \"spell\", \"spelling\", \"spelled\", \"correct\", \"corrected\", \"correction\",\n",
        "    \"this\", \"is\", \"an\", \"example\", \"test\", \"noisy\", \"channel\", \"model\",\n",
        "    \"hello\", \"world\", \"python\", \"programming\", \"language\"\n",
        "]\n",
        "\n",
        "# Add repeats to simulate frequency differences\n",
        "toy_corpus += [\"the\"] * 20 + [\"spelling\"] * 8 + [\"correct\"] * 10 + [\"python\"] * 6 + [\"programming\"] * 4\n",
        "\n",
        "WORD_FREQ = Counter(toy_corpus)\n",
        "VOCAB = set(WORD_FREQ.keys())\n",
        "\n",
        "def closest_by_edit_distance(word, vocab, freq, max_dist=2):\n",
        "    \"\"\"Return candidate corrections within max edit distance\"\"\"\n",
        "    results = []\n",
        "    for w in vocab:\n",
        "        d = edit_distance(word, w)\n",
        "        if d <= max_dist:   # <-- d is int, max_dist must also be int\n",
        "            results.append((w, d))\n",
        "    results.sort(key=lambda x: (x[1], -freq[x[0]], x[0]))\n",
        "    return results\n",
        "\n",
        "def noisy_channel_correction(word, vocab, freq, max_dist=2, alpha=1.0):\n",
        "    \"\"\"Rank candidates using noisy channel\"\"\"\n",
        "    candidates = closest_by_edit_distance(word, vocab, freq, max_dist)\n",
        "    if not candidates:\n",
        "        return word  # no correction found\n",
        "\n",
        "    V = len(vocab)\n",
        "    total = sum(freq.values()) + V  # add-one smoothing\n",
        "\n",
        "    best_cand, best_score = word, 0\n",
        "    for cand, dist in candidates:\n",
        "        prior = (freq[cand] + 1) / total\n",
        "        likelihood = math.exp(-alpha * dist)\n",
        "        score = prior * likelihood\n",
        "        if score > best_score:\n",
        "            best_cand, best_score = cand, score\n",
        "    return best_cand\n",
        "\n",
        "def correct_word(word, vocab, freq, max_dist=2, alpha=1.0):\n",
        "    \"\"\"Return the single best correction candidate for `word`.\"\"\"\n",
        "    ranked = noisy_channel_correction(word,vocab, freq, max_dist=max_dist, alpha=alpha)\n",
        "    return ranked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "HU1MhmV1pzMk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU1MhmV1pzMk",
        "outputId": "68401c9d-4599-4159-ada0-e20cdc3c2f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speling    -> spelling\n",
            "korrectud  -> corrected\n",
            "pythn      -> python\n",
            "thee       -> the\n",
            "langauge   -> language\n",
            "spel       -> spell\n"
          ]
        }
      ],
      "source": [
        "examples = [\"speling\", \"korrectud\", \"pythn\", \"thee\", \"langauge\", \"spel\"]\n",
        "for w in examples:\n",
        "    print(f\"{w:10s} -> {correct_word(w, VOCAB, WORD_FREQ)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PiEvgz7ip5Nw",
      "metadata": {
        "id": "PiEvgz7ip5Nw"
      },
      "source": [
        "Now, putting everything together, we will apply this to our email data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "nLxm9f9ep3H7",
      "metadata": {
        "id": "nLxm9f9ep3H7"
      },
      "outputs": [],
      "source": [
        "# --- Build corpus vocab from all emails ---\n",
        "all_tokens = []\n",
        "for email in email_data:\n",
        "    norm = normalize_email_text(email[\"body\"])\n",
        "    toks = word_tokenize(norm)\n",
        "    all_tokens.extend(t.lower() for t in toks if t.isalnum())\n",
        "\n",
        "WORD_FREQ = Counter(all_tokens)\n",
        "VOCAB = set(WORD_FREQ.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8zwCFwKes_3h",
      "metadata": {
        "id": "8zwCFwKes_3h"
      },
      "outputs": [],
      "source": [
        "# --- Full pipeline for one email ---\n",
        "def process_email(email_body: str):\n",
        "    norm = normalize_email_text(email_body)\n",
        "    no_stop = remove_stopwords(norm)\n",
        "    morph = morphological_analysis(no_stop)\n",
        "    corrected_tokens = [\n",
        "        noisy_channel_correction(w, VOCAB, WORD_FREQ, max_dist=2, alpha=1.0)\n",
        "        for w in word_tokenize(no_stop)\n",
        "    ]\n",
        "    return {\n",
        "        \"normalised\": norm,\n",
        "        \"no_stop\": no_stop,\n",
        "        \"morphology\": morph,\n",
        "        \"spell_corrected\": corrected_tokens,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "YOXYVciRtDYE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOXYVciRtDYE",
        "outputId": "7fe77906-d35e-4086-ee8e-b68ee53d0fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Processing completed for: 33.33%\n",
            "1\n",
            "Processing completed for: 66.67%\n",
            "2\n",
            "Processing completed for: 100.00%\n",
            "{'morphology': {'lemmatization': [('stacey', 'stacey'),\n",
            "                                  ('anderson', 'anderson'),\n",
            "                                  ('diane', 'diane'),\n",
            "                                  ('gossett', 'gossett'),\n",
            "                                  ('jeffrey', 'jeffrey'),\n",
            "                                  ('white', 'white'),\n",
            "                                  ('ey', 'ey'),\n",
            "                                  ('murphy', 'murphy'),\n",
            "                                  ('melissa', 'melissa'),\n",
            "                                  ('hall', 'hall'),\n",
            "                                  ('todd', 'todd'),\n",
            "                                  ('sweeney', 'sweeney'),\n",
            "                                  ('kevin', 'kevin'),\n",
            "                                  ('cc', 'cc'),\n",
            "                                  ('evelyn', 'evelyn'),\n",
            "                                  ('baxter', 'baxter'),\n",
            "                                  ('bryce', 'bryce'),\n",
            "                                  ('wynne', 'wynne'),\n",
            "                                  ('rita', 'rita'),\n",
            "                                  ('laurel', 'laurel'),\n",
            "                                  ('alonso', 'alonso'),\n",
            "                                  ('tom', 'tom'),\n",
            "                                  ('aronowitz', 'aronowitz'),\n",
            "                                  ('alan', 'alan'),\n",
            "                                  ('bailey', 'bailey'),\n",
            "                                  ('susan', 'susan'),\n",
            "                                  ('lanagan', 'lanagan'),\n",
            "                                  ('cyndie', 'cyndie'),\n",
            "                                  ('baughman', 'baughman'),\n",
            "                                  ('edward', 'edward'),\n",
            "                                  ('belden', 'belden'),\n",
            "                                  ('tim', 'tim'),\n",
            "                                  ('bishop', 'bishop'),\n",
            "                                  ('serena', 'serena'),\n",
            "                                  ('brackett', 'brackett'),\n",
            "                                  ('ebbie', 'ebbie'),\n",
            "                                  ('bradford', 'bradford'),\n",
            "                                  ('william', 'william'),\n",
            "                                  ('browning', 'brown'),\n",
            "                                  ('mary', 'mary'),\n",
            "                                  ('nell', 'nell'),\n",
            "                                  ('bruce', 'bruce'),\n",
            "                                  ('james', 'jam'),\n",
            "                                  ('bruce', 'bruce'),\n",
            "                                  ('ichelle', 'ichelle'),\n",
            "                                  ('bruce', 'bruce'),\n",
            "                                  ('robert', 'robert'),\n",
            "                                  ('buerkle', 'buerkle'),\n",
            "                                  ('jim', 'jim'),\n",
            "                                  ('calger', 'calger'),\n",
            "                                  ('christopher', 'christopher'),\n",
            "                                  ('carrington', 'carrington'),\n",
            "                                  ('lara', 'lara'),\n",
            "                                  ('considine', 'considine'),\n",
            "                                  ('keith', 'keith'),\n",
            "                                  ('cordova', 'cordova'),\n",
            "                                  ('karen', 'karen'),\n",
            "                                  ('crandall', 'crandall'),\n",
            "                                  ('sean', 'sean'),\n",
            "                                  ('cutsforth', 'cutsforth'),\n",
            "                                  ('diamond', 'diamond'),\n",
            "                                  ('russell', 'russell'),\n",
            "                                  ('dunton', 'dunton'),\n",
            "                                  ('heather', 'heather'),\n",
            "                                  ('edison', 'edison'),\n",
            "                                  ('susan', 'susan'),\n",
            "                                  ('elafandi', 'elafandi'),\n",
            "                                  ('mo', 'mo'),\n",
            "                                  ('fischer', 'fischer'),\n",
            "                                  ('mark', 'mark'),\n",
            "                                  ('flores', 'flores'),\n",
            "                                  ('nony', 'nony'),\n",
            "                                  ('fondren', 'fondren'),\n",
            "                                  ('mark', 'mark'),\n",
            "                                  ('gorny', 'gorny'),\n",
            "                                  ('vladimir', 'vladimir'),\n",
            "                                  ('gorte', 'gorte'),\n",
            "                                  ('david', 'david'),\n",
            "                                  ('gresham', 'gresham'),\n",
            "                                  ('wayne', 'wayne'),\n",
            "                                  ('hagelmann', 'hagelmann'),\n",
            "                                  ('bjorn', 'bjorn'),\n",
            "                                  ('hall', 'hall'),\n",
            "                                  ('steve', 'steve'),\n",
            "                                  ('legal', 'legal'),\n",
            "                                  ('harkness', 'harkness'),\n",
            "                                  ('cynthia', 'cynthia'),\n",
            "                                  ('hendry', 'hendry'),\n",
            "                                  ('brent', 'brent'),\n",
            "                                  ('johnston', 'johnston'),\n",
            "                                  ('greg', 'greg'),\n",
            "                                  ('keohane', 'keohane'),\n",
            "                                  ('peter', 'peter'),\n",
            "                                  ('lindeman', 'lindeman'),\n",
            "                                  ('cheryl', 'cheryl'),\n",
            "                                  ('little', 'little'),\n",
            "                                  ('kelli', 'kelli'),\n",
            "                                  ('llory', 'llory'),\n",
            "                                  ('chris', 'chris'),\n",
            "                                  ('mann', 'mann'),\n",
            "                                  ('kay', 'kay'),\n",
            "                                  ('mcginnis', 'mcginnis'),\n",
            "                                  ('stephanie', 'stephanie'),\n",
            "                                  ('mcgrory', 'mcgrory'),\n",
            "                                  ('robert', 'robert'),\n",
            "                                  ('mcmichael', 'mcmichael'),\n",
            "                                  ('ed', 'ed'),\n",
            "                                  ('miller', 'miller'),\n",
            "                                  ('asset', 'asset'),\n",
            "                                  ('mktg', 'mktg'),\n",
            "                                  ('moore', 'moore'),\n",
            "                                  ('janet', 'janet'),\n",
            "                                  ('moran', 'moran'),\n",
            "                                  ('tom', 'tom'),\n",
            "                                  ('murphy', 'murphy'),\n",
            "                                  ('n', 'n'),\n",
            "                                  ('murray', 'murray'),\n",
            "                                  ('julia', 'julia'),\n",
            "                                  ('nemec', 'nemec'),\n",
            "                                  ('gerald', 'gerald'),\n",
            "                                  ('ogden', 'ogden'),\n",
            "                                  ('mary', 'mary'),\n",
            "                                  ('otto', 'otto'),\n",
            "                                  ('randy', 'randy'),\n",
            "                                  ('page', 'page'),\n",
            "                                  ('jonalan', 'jonalan'),\n",
            "                                  ('ostlethwaite', 'ostlethwaite'),\n",
            "                                  ('john', 'john'),\n",
            "                                  ('prejean', 'prejean'),\n",
            "                                  ('frank', 'frank'),\n",
            "                                  ('presto', 'presto'),\n",
            "                                  ('kevin', 'kevin'),\n",
            "                                  ('puchot', 'puchot'),\n",
            "                                  ('paul', 'paul'),\n",
            "                                  ('en', 'en'),\n",
            "                                  ('dale', 'dale'),\n",
            "                                  ('richter', 'richter'),\n",
            "                                  ('brad', 'brad'),\n",
            "                                  ('richter', 'richter'),\n",
            "                                  ('jeff', 'jeff'),\n",
            "                                  ('robison', 'robison'),\n",
            "                                  ('michael', 'michael'),\n",
            "                                  ('rohauer', 'rohauer'),\n",
            "                                  ('rosman', 'rosman'),\n",
            "                                  ('stewart', 'stewart'),\n",
            "                                  ('runswick', 'runswick'),\n",
            "                                  ('stacy', 'stacy'),\n",
            "                                  ('sacks', 'sack'),\n",
            "                                  ('edward', 'edward'),\n",
            "                                  ('scholtes', 'scholtes'),\n",
            "                                  ('diana', 'diana'),\n",
            "                                  ('ton', 'ton'),\n",
            "                                  ('sara', 'sara'),\n",
            "                                  ('simons', 'simon'),\n",
            "                                  ('paul', 'paul'),\n",
            "                                  ('swinney', 'swinney'),\n",
            "                                  ('john', 'john'),\n",
            "                                  ('thapar', 'thapar'),\n",
            "                                  ('raj', 'raj'),\n",
            "                                  ('theriot', 'theriot'),\n",
            "                                  ('kim', 'kim'),\n",
            "                                  ('jake', 'jake'),\n",
            "                                  ('thome', 'thome'),\n",
            "                                  ('stephen', 'stephen'),\n",
            "                                  ('tricoli', 'tricoli'),\n",
            "                                  ('carl', 'carl'),\n",
            "                                  ('van', 'van'),\n",
            "                                  ('hooser', 'hooser'),\n",
            "                                  ('steve', 'steve'),\n",
            "                                  ('wente', 'wente'),\n",
            "                                  ('laura', 'laura'),\n",
            "                                  ('lson', 'lson'),\n",
            "                                  ('shona', 'shona'),\n",
            "                                  ('winfree', 'winfree'),\n",
            "                                  ('woodland', 'woodland'),\n",
            "                                  ('andrea', 'andrea'),\n",
            "                                  ('yoder', 'yoder'),\n",
            "                                  ('christian', 'christian'),\n",
            "                                  ('attached', 'attach'),\n",
            "                                  ('daily', 'daily'),\n",
            "                                  ('termination', 'termination'),\n",
            "                                  ('list', 'list'),\n",
            "                                  ('january', 'january'),\n",
            "                                  ('25', '25'),\n",
            "                                  ('well', 'well'),\n",
            "                                  ('termination', 'termination'),\n",
            "                                  ('log', 'log'),\n",
            "                                  ('incorporates', 'incorporate'),\n",
            "                                  ('terminations', 'termination'),\n",
            "                                  ('received', 'receive'),\n",
            "                                  ('ary', 'ary'),\n",
            "                                  ('25', '25'),\n",
            "                                  ('following', 'follow'),\n",
            "                                  ('previously', 'previously'),\n",
            "                                  ('master', 'master'),\n",
            "                                  ('termination', 'termination'),\n",
            "                                  ('log', 'log'),\n",
            "                                  ('en', 'en'),\n",
            "                                  ('marked', 'mark'),\n",
            "                                  ('valid', 'valid'),\n",
            "                                  ('termination', 'termination'),\n",
            "                                  ('atlantic', 'atlantic'),\n",
            "                                  ('coast', 'coast'),\n",
            "                                  ('fibers', 'fiber'),\n",
            "                                  ('transactions', 'transaction'),\n",
            "                                  ('power', 'power'),\n",
            "                                  ('agreement', 'agreement'),\n",
            "                                  ('public', 'public'),\n",
            "                                  ('utility', 'utility'),\n",
            "                                  ('district', 'district'),\n",
            "                                  ('1', '1'),\n",
            "                                  ('chelan', 'chelan'),\n",
            "                                  ('01', '01'),\n",
            "                                  ('connect', 'connect'),\n",
            "                                  ('energy', 'energy'),\n",
            "                                  ('services', 'service'),\n",
            "                                  ('agreement', 'agreement'),\n",
            "                                  ('ngl', 'ngl'),\n",
            "                                  ('supply', 'supply'),\n",
            "                                  ('including', 'include'),\n",
            "                                  ('financial', 'financial'),\n",
            "                                  ('ansactions', 'ansactions'),\n",
            "                                  ('referenced', 'reference'),\n",
            "                                  ('energy', 'energy'),\n",
            "                                  ('partners', 'partner'),\n",
            "                                  ('division', 'division'),\n",
            "                                  ('ngl', 'ngl'),\n",
            "                                  ('supply', 'supply'),\n",
            "                                  ('plains', 'plain'),\n",
            "                                  ('marketing', 'market'),\n",
            "                                  ('plains', 'plain'),\n",
            "                                  ('marketing', 'market'),\n",
            "                                  ('stephanie', 'stephanie'),\n",
            "                                  ('panus', 'panus'),\n",
            "                                  ('enron', 'enron'),\n",
            "                                  ('wholesale', 'wholesale'),\n",
            "                                  ('services', 'service'),\n",
            "                                  ('ph', 'ph'),\n",
            "                                  ('fax', 'fax')],\n",
            "                'stemming': [('stacey', 'stacey'),\n",
            "                             ('anderson', 'anderson'),\n",
            "                             ('diane', 'dian'),\n",
            "                             ('gossett', 'gossett'),\n",
            "                             ('jeffrey', 'jeffrey'),\n",
            "                             ('white', 'white'),\n",
            "                             ('ey', 'ey'),\n",
            "                             ('murphy', 'murphi'),\n",
            "                             ('melissa', 'melissa'),\n",
            "                             ('hall', 'hall'),\n",
            "                             ('todd', 'todd'),\n",
            "                             ('sweeney', 'sweeney'),\n",
            "                             ('kevin', 'kevin'),\n",
            "                             ('cc', 'cc'),\n",
            "                             ('evelyn', 'evelyn'),\n",
            "                             ('baxter', 'baxter'),\n",
            "                             ('bryce', 'bryce'),\n",
            "                             ('wynne', 'wynn'),\n",
            "                             ('rita', 'rita'),\n",
            "                             ('laurel', 'laurel'),\n",
            "                             ('alonso', 'alonso'),\n",
            "                             ('tom', 'tom'),\n",
            "                             ('aronowitz', 'aronowitz'),\n",
            "                             ('alan', 'alan'),\n",
            "                             ('bailey', 'bailey'),\n",
            "                             ('susan', 'susan'),\n",
            "                             ('lanagan', 'lanagan'),\n",
            "                             ('cyndie', 'cyndi'),\n",
            "                             ('baughman', 'baughman'),\n",
            "                             ('edward', 'edward'),\n",
            "                             ('belden', 'belden'),\n",
            "                             ('tim', 'tim'),\n",
            "                             ('bishop', 'bishop'),\n",
            "                             ('serena', 'serena'),\n",
            "                             ('brackett', 'brackett'),\n",
            "                             ('ebbie', 'ebbi'),\n",
            "                             ('bradford', 'bradford'),\n",
            "                             ('william', 'william'),\n",
            "                             ('browning', 'brown'),\n",
            "                             ('mary', 'mari'),\n",
            "                             ('nell', 'nell'),\n",
            "                             ('bruce', 'bruce'),\n",
            "                             ('james', 'jame'),\n",
            "                             ('bruce', 'bruce'),\n",
            "                             ('ichelle', 'ichel'),\n",
            "                             ('bruce', 'bruce'),\n",
            "                             ('robert', 'robert'),\n",
            "                             ('buerkle', 'buerkl'),\n",
            "                             ('jim', 'jim'),\n",
            "                             ('calger', 'calger'),\n",
            "                             ('christopher', 'christoph'),\n",
            "                             ('carrington', 'carrington'),\n",
            "                             ('lara', 'lara'),\n",
            "                             ('considine', 'considin'),\n",
            "                             ('keith', 'keith'),\n",
            "                             ('cordova', 'cordova'),\n",
            "                             ('karen', 'karen'),\n",
            "                             ('crandall', 'crandal'),\n",
            "                             ('sean', 'sean'),\n",
            "                             ('cutsforth', 'cutsforth'),\n",
            "                             ('diamond', 'diamond'),\n",
            "                             ('russell', 'russel'),\n",
            "                             ('dunton', 'dunton'),\n",
            "                             ('heather', 'heather'),\n",
            "                             ('edison', 'edison'),\n",
            "                             ('susan', 'susan'),\n",
            "                             ('elafandi', 'elafandi'),\n",
            "                             ('mo', 'mo'),\n",
            "                             ('fischer', 'fischer'),\n",
            "                             ('mark', 'mark'),\n",
            "                             ('flores', 'flore'),\n",
            "                             ('nony', 'noni'),\n",
            "                             ('fondren', 'fondren'),\n",
            "                             ('mark', 'mark'),\n",
            "                             ('gorny', 'gorni'),\n",
            "                             ('vladimir', 'vladimir'),\n",
            "                             ('gorte', 'gort'),\n",
            "                             ('david', 'david'),\n",
            "                             ('gresham', 'gresham'),\n",
            "                             ('wayne', 'wayn'),\n",
            "                             ('hagelmann', 'hagelmann'),\n",
            "                             ('bjorn', 'bjorn'),\n",
            "                             ('hall', 'hall'),\n",
            "                             ('steve', 'steve'),\n",
            "                             ('legal', 'legal'),\n",
            "                             ('harkness', 'hark'),\n",
            "                             ('cynthia', 'cynthia'),\n",
            "                             ('hendry', 'hendri'),\n",
            "                             ('brent', 'brent'),\n",
            "                             ('johnston', 'johnston'),\n",
            "                             ('greg', 'greg'),\n",
            "                             ('keohane', 'keohan'),\n",
            "                             ('peter', 'peter'),\n",
            "                             ('lindeman', 'lindeman'),\n",
            "                             ('cheryl', 'cheryl'),\n",
            "                             ('little', 'littl'),\n",
            "                             ('kelli', 'kelli'),\n",
            "                             ('llory', 'llori'),\n",
            "                             ('chris', 'chri'),\n",
            "                             ('mann', 'mann'),\n",
            "                             ('kay', 'kay'),\n",
            "                             ('mcginnis', 'mcginni'),\n",
            "                             ('stephanie', 'stephani'),\n",
            "                             ('mcgrory', 'mcgrori'),\n",
            "                             ('robert', 'robert'),\n",
            "                             ('mcmichael', 'mcmichael'),\n",
            "                             ('ed', 'ed'),\n",
            "                             ('miller', 'miller'),\n",
            "                             ('asset', 'asset'),\n",
            "                             ('mktg', 'mktg'),\n",
            "                             ('moore', 'moor'),\n",
            "                             ('janet', 'janet'),\n",
            "                             ('moran', 'moran'),\n",
            "                             ('tom', 'tom'),\n",
            "                             ('murphy', 'murphi'),\n",
            "                             ('n', 'n'),\n",
            "                             ('murray', 'murray'),\n",
            "                             ('julia', 'julia'),\n",
            "                             ('nemec', 'nemec'),\n",
            "                             ('gerald', 'gerald'),\n",
            "                             ('ogden', 'ogden'),\n",
            "                             ('mary', 'mari'),\n",
            "                             ('otto', 'otto'),\n",
            "                             ('randy', 'randi'),\n",
            "                             ('page', 'page'),\n",
            "                             ('jonalan', 'jonalan'),\n",
            "                             ('ostlethwaite', 'ostlethwait'),\n",
            "                             ('john', 'john'),\n",
            "                             ('prejean', 'prejean'),\n",
            "                             ('frank', 'frank'),\n",
            "                             ('presto', 'presto'),\n",
            "                             ('kevin', 'kevin'),\n",
            "                             ('puchot', 'puchot'),\n",
            "                             ('paul', 'paul'),\n",
            "                             ('en', 'en'),\n",
            "                             ('dale', 'dale'),\n",
            "                             ('richter', 'richter'),\n",
            "                             ('brad', 'brad'),\n",
            "                             ('richter', 'richter'),\n",
            "                             ('jeff', 'jeff'),\n",
            "                             ('robison', 'robison'),\n",
            "                             ('michael', 'michael'),\n",
            "                             ('rohauer', 'rohauer'),\n",
            "                             ('rosman', 'rosman'),\n",
            "                             ('stewart', 'stewart'),\n",
            "                             ('runswick', 'runswick'),\n",
            "                             ('stacy', 'staci'),\n",
            "                             ('sacks', 'sack'),\n",
            "                             ('edward', 'edward'),\n",
            "                             ('scholtes', 'scholt'),\n",
            "                             ('diana', 'diana'),\n",
            "                             ('ton', 'ton'),\n",
            "                             ('sara', 'sara'),\n",
            "                             ('simons', 'simon'),\n",
            "                             ('paul', 'paul'),\n",
            "                             ('swinney', 'swinney'),\n",
            "                             ('john', 'john'),\n",
            "                             ('thapar', 'thapar'),\n",
            "                             ('raj', 'raj'),\n",
            "                             ('theriot', 'theriot'),\n",
            "                             ('kim', 'kim'),\n",
            "                             ('jake', 'jake'),\n",
            "                             ('thome', 'thome'),\n",
            "                             ('stephen', 'stephen'),\n",
            "                             ('tricoli', 'tricoli'),\n",
            "                             ('carl', 'carl'),\n",
            "                             ('van', 'van'),\n",
            "                             ('hooser', 'hooser'),\n",
            "                             ('steve', 'steve'),\n",
            "                             ('wente', 'went'),\n",
            "                             ('laura', 'laura'),\n",
            "                             ('lson', 'lson'),\n",
            "                             ('shona', 'shona'),\n",
            "                             ('winfree', 'winfre'),\n",
            "                             ('woodland', 'woodland'),\n",
            "                             ('andrea', 'andrea'),\n",
            "                             ('yoder', 'yoder'),\n",
            "                             ('christian', 'christian'),\n",
            "                             ('attached', 'attach'),\n",
            "                             ('daily', 'daili'),\n",
            "                             ('termination', 'termin'),\n",
            "                             ('list', 'list'),\n",
            "                             ('january', 'januari'),\n",
            "                             ('25', '25'),\n",
            "                             ('well', 'well'),\n",
            "                             ('termination', 'termin'),\n",
            "                             ('log', 'log'),\n",
            "                             ('incorporates', 'incorpor'),\n",
            "                             ('terminations', 'termin'),\n",
            "                             ('received', 'receiv'),\n",
            "                             ('ary', 'ari'),\n",
            "                             ('25', '25'),\n",
            "                             ('following', 'follow'),\n",
            "                             ('previously', 'previous'),\n",
            "                             ('master', 'master'),\n",
            "                             ('termination', 'termin'),\n",
            "                             ('log', 'log'),\n",
            "                             ('en', 'en'),\n",
            "                             ('marked', 'mark'),\n",
            "                             ('valid', 'valid'),\n",
            "                             ('termination', 'termin'),\n",
            "                             ('atlantic', 'atlant'),\n",
            "                             ('coast', 'coast'),\n",
            "                             ('fibers', 'fiber'),\n",
            "                             ('transactions', 'transact'),\n",
            "                             ('power', 'power'),\n",
            "                             ('agreement', 'agreement'),\n",
            "                             ('public', 'public'),\n",
            "                             ('utility', 'util'),\n",
            "                             ('district', 'district'),\n",
            "                             ('1', '1'),\n",
            "                             ('chelan', 'chelan'),\n",
            "                             ('01', '01'),\n",
            "                             ('connect', 'connect'),\n",
            "                             ('energy', 'energi'),\n",
            "                             ('services', 'servic'),\n",
            "                             ('agreement', 'agreement'),\n",
            "                             ('ngl', 'ngl'),\n",
            "                             ('supply', 'suppli'),\n",
            "                             ('including', 'includ'),\n",
            "                             ('financial', 'financi'),\n",
            "                             ('ansactions', 'ansact'),\n",
            "                             ('referenced', 'referenc'),\n",
            "                             ('energy', 'energi'),\n",
            "                             ('partners', 'partner'),\n",
            "                             ('division', 'divis'),\n",
            "                             ('ngl', 'ngl'),\n",
            "                             ('supply', 'suppli'),\n",
            "                             ('plains', 'plain'),\n",
            "                             ('marketing', 'market'),\n",
            "                             ('plains', 'plain'),\n",
            "                             ('marketing', 'market'),\n",
            "                             ('stephanie', 'stephani'),\n",
            "                             ('panus', 'panu'),\n",
            "                             ('enron', 'enron'),\n",
            "                             ('wholesale', 'wholesal'),\n",
            "                             ('services', 'servic'),\n",
            "                             ('ph', 'ph'),\n",
            "                             ('fax', 'fax')]},\n",
            " 'no_stop': 'stacey anderson diane gossett jeffrey white ey murphy melissa '\n",
            "            'hall todd sweeney kevin cc evelyn baxter bryce wynne rita laurel '\n",
            "            'alonso tom aronowitz alan bailey susan lanagan cyndie baughman '\n",
            "            'edward belden tim bishop serena brackett ebbie bradford william '\n",
            "            'browning mary nell bruce james bruce ichelle bruce robert buerkle '\n",
            "            'jim calger christopher carrington lara considine keith cordova '\n",
            "            'karen crandall sean cutsforth diamond russell dunton heather '\n",
            "            'edison susan elafandi mo fischer mark flores nony fondren mark '\n",
            "            'gorny vladimir gorte david gresham wayne hagelmann bjorn hall '\n",
            "            'steve legal harkness cynthia hendry brent johnston greg keohane '\n",
            "            'peter lindeman cheryl little kelli llory chris mann kay mcginnis '\n",
            "            'stephanie mcgrory robert mcmichael ed miller asset mktg moore '\n",
            "            'janet moran tom murphy n murray julia nemec gerald ogden mary '\n",
            "            'otto randy page jonalan ostlethwaite john prejean frank presto '\n",
            "            'kevin puchot paul en dale richter brad richter jeff robison '\n",
            "            'michael rohauer rosman stewart runswick stacy sacks edward '\n",
            "            'scholtes diana ton sara simons paul swinney john thapar raj '\n",
            "            'theriot kim jake thome stephen tricoli carl van hooser steve '\n",
            "            'wente laura lson shona winfree woodland andrea yoder christian '\n",
            "            'attached daily termination list january 25 well termination log '\n",
            "            'incorporates terminations received ary 25 following previously '\n",
            "            'master termination log en marked valid termination atlantic coast '\n",
            "            'fibers transactions power agreement public utility district 1 '\n",
            "            'chelan 01 connect energy services agreement ngl supply including '\n",
            "            'financial ansactions referenced energy partners division ngl '\n",
            "            'supply plains marketing plains marketing stephanie panus enron '\n",
            "            'wholesale services ph fax',\n",
            " 'normalised': 'to:=09richardson, stacey; anderson, diane; gossett, jeffrey '\n",
            "               'c.; white, stac= ey w.; murphy, melissa; hall, d. todd; '\n",
            "               'sweeney, kevin cc:=09aucoin, evelyn; baxter, bryce; wynne, '\n",
            "               'rita to:=09adams, laurel; alonso, tom; aronowitz, alan; '\n",
            "               'bailey, susan; balfour-f= lanagan, cyndie; baughman, edward; '\n",
            "               'belden, tim; bishop, serena; brackett, d= ebbie r.; bradford, '\n",
            "               'william s.; browning, mary nell; bruce, james; bruce, m= '\n",
            "               'ichelle; bruce, robert; buerkle, jim; calger, christopher f.; '\n",
            "               'carrington, c= lara; considine, keith; cordova, karen a.; '\n",
            "               'crandall, sean; cutsforth, diane= ; diamond, russell; dunton, '\n",
            "               'heather; edison, susan; elafandi, mo; fischer, = mark; flores, '\n",
            "               'nony; fondren, mark; gorny, vladimir; gorte, david; gresham, = '\n",
            "               'wayne; hagelmann, bjorn; hall, steve c. (legal); harkness, '\n",
            "               'cynthia; hendry,= brent; johnston, greg; keohane, peter; '\n",
            "               'lindeman, cheryl; little, kelli; ma= llory, chris; mann, kay; '\n",
            "               'mcginnis, stephanie; mcgrory, robert; mcmichael jr= ., ed; '\n",
            "               'miller, don (asset mktg); moore, janet h.; moran, tom; murphy, '\n",
            "               'harla= n; murray, julia; nemec, gerald; ogden, mary; otto, '\n",
            "               'randy; page, jonalan; p= ostlethwaite, john; prejean, frank; '\n",
            "               'presto, kevin m.; puchot, paul; rasmuss= en, dale; richter, '\n",
            "               'brad; richter, jeff; robison, michael a.; rohauer, tanya= ; '\n",
            "               'rosman, stewart; runswick, stacy; sacks, edward; scholtes, '\n",
            "               'diana; shackle= ton, sara; simons, paul; swinney, john; '\n",
            "               'thapar, raj; theriot, kim s.; thoma= s, jake; thome, stephen; '\n",
            "               'tricoli, carl; van hooser, steve; wente, laura; wi= lson, '\n",
            "               \"shona; winfree, o'neal d.; woodland, andrea; yoder, christian \"\n",
            "               'attached is the daily termination list for january 25 as well '\n",
            "               'as the master= termination log, which incorporates all '\n",
            "               'terminations received through janu= ary 25. =20 the following '\n",
            "               'were previously on the master termination log and have now be= '\n",
            "               'en marked as \"y\" for a valid termination: atlantic coast '\n",
            "               'fibers, inc.=09=09=09ena=09=09pulp/paper transactions '\n",
            "               'cnc-containers corporation=09=09=09epmi=09=09master power '\n",
            "               'agreement public utility district no. 1 of chelan '\n",
            "               'county=09epmi=09=09deal no. 757497.= 01 connect energy '\n",
            "               'services, inc.=09=09=09ena=09=09liquids agreement ngl supply, '\n",
            "               'inc. (including premier=09=09ena/egli=09physical & financial '\n",
            "               'tr= ansactions referenced energy partners, a division of ngl '\n",
            "               'supply, inc.) plains marketing, l.p.=09=09=09=09erac=09=09deal '\n",
            "               'no. qg4563.1 plains marketing, l.p.=09=09=09=09erac=09=09deal '\n",
            "               'no. qg4482.2 stephanie panus enron wholesale services ph: fax:',\n",
            " 'spell_corrected': ['stacey',\n",
            "                     'anderson',\n",
            "                     'diane',\n",
            "                     'gossett',\n",
            "                     'jeffrey',\n",
            "                     'with',\n",
            "                     'to',\n",
            "                     'murphy',\n",
            "                     'melissa',\n",
            "                     'have',\n",
            "                     'to',\n",
            "                     'sweeney',\n",
            "                     'kevin',\n",
            "                     'cc',\n",
            "                     'evelyn',\n",
            "                     'after',\n",
            "                     'price',\n",
            "                     'wynne',\n",
            "                     'with',\n",
            "                     'laurel',\n",
            "                     'also',\n",
            "                     'to',\n",
            "                     'aronowitz',\n",
            "                     'can',\n",
            "                     'bailey',\n",
            "                     'susan',\n",
            "                     'flanagan',\n",
            "                     'cyndie',\n",
            "                     'baughman',\n",
            "                     'edward',\n",
            "                     'been',\n",
            "                     'the',\n",
            "                     'bishop',\n",
            "                     'seen',\n",
            "                     'brackett',\n",
            "                     'debbie',\n",
            "                     'bradford',\n",
            "                     'william',\n",
            "                     'browning',\n",
            "                     'are',\n",
            "                     'will',\n",
            "                     'bruce',\n",
            "                     'james',\n",
            "                     'bruce',\n",
            "                     'michelle',\n",
            "                     'bruce',\n",
            "                     'robert',\n",
            "                     'buerkle',\n",
            "                     'i',\n",
            "                     'calger',\n",
            "                     'christopher',\n",
            "                     'carrington',\n",
            "                     'are',\n",
            "                     'considine',\n",
            "                     'with',\n",
            "                     'cordova',\n",
            "                     'are',\n",
            "                     'crandall',\n",
            "                     'can',\n",
            "                     'cutsforth',\n",
            "                     'diamond',\n",
            "                     'russell',\n",
            "                     'denton',\n",
            "                     'heather',\n",
            "                     'edison',\n",
            "                     'susan',\n",
            "                     'elafandi',\n",
            "                     'to',\n",
            "                     'fischer',\n",
            "                     'mark',\n",
            "                     'forms',\n",
            "                     'on',\n",
            "                     'fondren',\n",
            "                     'mark',\n",
            "                     'going',\n",
            "                     'vladimir',\n",
            "                     'more',\n",
            "                     'david',\n",
            "                     'gresham',\n",
            "                     'want',\n",
            "                     'hagelmann',\n",
            "                     'john',\n",
            "                     'have',\n",
            "                     'steve',\n",
            "                     'legal',\n",
            "                     'harkness',\n",
            "                     'cynthia',\n",
            "                     'hendry',\n",
            "                     'been',\n",
            "                     'johnson',\n",
            "                     'are',\n",
            "                     'keohane',\n",
            "                     'power',\n",
            "                     'lindeman',\n",
            "                     'cheryl',\n",
            "                     'little',\n",
            "                     'well',\n",
            "                     'larry',\n",
            "                     'this',\n",
            "                     'and',\n",
            "                     'a',\n",
            "                     'mcginnis',\n",
            "                     'stephanie',\n",
            "                     'mcgrory',\n",
            "                     'robert',\n",
            "                     'michael',\n",
            "                     'to',\n",
            "                     'miller',\n",
            "                     'asset',\n",
            "                     'mktg',\n",
            "                     'more',\n",
            "                     'want',\n",
            "                     'more',\n",
            "                     'to',\n",
            "                     'murphy',\n",
            "                     'i',\n",
            "                     'murray',\n",
            "                     'julie',\n",
            "                     'need',\n",
            "                     'gerald',\n",
            "                     'order',\n",
            "                     'are',\n",
            "                     'to',\n",
            "                     'and',\n",
            "                     'have',\n",
            "                     'jonathan',\n",
            "                     'postlethwaite',\n",
            "                     'john',\n",
            "                     'prejean',\n",
            "                     'frank',\n",
            "                     'presto',\n",
            "                     'kevin',\n",
            "                     'puchot',\n",
            "                     'paul',\n",
            "                     'to',\n",
            "                     'have',\n",
            "                     'richter',\n",
            "                     'had',\n",
            "                     'richter',\n",
            "                     'jeff',\n",
            "                     'robin',\n",
            "                     'michael',\n",
            "                     'rohauer',\n",
            "                     'rosman',\n",
            "                     'start',\n",
            "                     'runswick',\n",
            "                     'stacey',\n",
            "                     'back',\n",
            "                     'edward',\n",
            "                     'scholtes',\n",
            "                     'tana',\n",
            "                     'to',\n",
            "                     'sara',\n",
            "                     'simons',\n",
            "                     'paul',\n",
            "                     'swinney',\n",
            "                     'john',\n",
            "                     'thapar',\n",
            "                     'a',\n",
            "                     'theriot',\n",
            "                     'i',\n",
            "                     'have',\n",
            "                     'the',\n",
            "                     'stephen',\n",
            "                     'tricoli',\n",
            "                     'are',\n",
            "                     'and',\n",
            "                     'house',\n",
            "                     'steve',\n",
            "                     'were',\n",
            "                     'laura',\n",
            "                     'on',\n",
            "                     'phone',\n",
            "                     'winfree',\n",
            "                     'woodland',\n",
            "                     'andrea',\n",
            "                     'your',\n",
            "                     'christian',\n",
            "                     'attached',\n",
            "                     'daily',\n",
            "                     'termination',\n",
            "                     'is',\n",
            "                     'january',\n",
            "                     'to',\n",
            "                     'will',\n",
            "                     'termination',\n",
            "                     'to',\n",
            "                     'incorporate',\n",
            "                     'termination',\n",
            "                     'received',\n",
            "                     'and',\n",
            "                     'to',\n",
            "                     'following',\n",
            "                     'previously',\n",
            "                     'master',\n",
            "                     'termination',\n",
            "                     'to',\n",
            "                     'to',\n",
            "                     'market',\n",
            "                     'david',\n",
            "                     'termination',\n",
            "                     'atlantic',\n",
            "                     'cost',\n",
            "                     'files',\n",
            "                     'transactions',\n",
            "                     'power',\n",
            "                     'agreement',\n",
            "                     'public',\n",
            "                     'utility',\n",
            "                     'district',\n",
            "                     'i',\n",
            "                     'clean',\n",
            "                     'to',\n",
            "                     'contact',\n",
            "                     'energy',\n",
            "                     'services',\n",
            "                     'agreement',\n",
            "                     'not',\n",
            "                     'supply',\n",
            "                     'including',\n",
            "                     'financial',\n",
            "                     'transactions',\n",
            "                     'referenced',\n",
            "                     'energy',\n",
            "                     'partners',\n",
            "                     'division',\n",
            "                     'not',\n",
            "                     'supply',\n",
            "                     'plan',\n",
            "                     'marketing',\n",
            "                     'plan',\n",
            "                     'marketing',\n",
            "                     'stephanie',\n",
            "                     'paul',\n",
            "                     'enron',\n",
            "                     'wholesale',\n",
            "                     'services',\n",
            "                     'the',\n",
            "                     'a']}\n"
          ]
        }
      ],
      "source": [
        "# --- Apply pipeline to all emails ---\n",
        "processed_emails = []\n",
        "for i, email in enumerate(email_data[:3]):\n",
        "    processed = process_email(email[\"body\"])\n",
        "    processed_emails.append({\n",
        "        \"thread_id\": email[\"thread_id\"],\n",
        "        \"subject\": email[\"subject\"],\n",
        "        \"processed\": processed\n",
        "    })\n",
        "    print(f\"{i}\")\n",
        "    print(f\"Processing completed for: {((i+1)/len(email_data[:3])*100):.2f}%\")\n",
        "\n",
        "# Example output\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(processed_emails[0][\"processed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "btrG83azi5ov",
      "metadata": {
        "id": "btrG83azi5ov"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0FZ6cZz_-QpQ",
      "metadata": {
        "id": "0FZ6cZz_-QpQ"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In this pipeline, we walked through the essential **lexical processing** steps for email data. We began with **text normalisation** to remove noise like headers, signatures, and quoted replies, making the email body consistent. Then we applied **tokenisation** and **stopword removal** to break the text into meaningful units and filter out high-frequency but semantically weak words. Through **morphological analysis (stemming and lemmatisation)** we reduced word variants to their base forms, improving vocabulary consistency. Finally, we integrated a **spell correction** module using edit distance and a noisy-channel model, ensuring typos are mapped to their most likely valid forms. Together, these steps transform raw, messy emails into a clean, structured, and standardised representation that is far more suitable for downstream tasks like summarisation, topic modelling, and semantic analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eHbPNKs0-q3r",
      "metadata": {
        "id": "eHbPNKs0-q3r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}